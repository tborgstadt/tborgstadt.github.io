<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xml" href="/feed.xslt.xml"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.1.6">Jekyll</generator><link href="http://blog.tomborgstadt.com//feed.xml" rel="self" type="application/atom+xml" /><link href="http://blog.tomborgstadt.com//" rel="alternate" type="text/html" /><updated>2017-10-13T21:51:41-05:00</updated><id>http://blog.tomborgstadt.com//</id><title>Tom Borgstadt</title><subtitle>Data Engineering, Data Mining, and Machine Learning</subtitle><author><name>Tom Borgstadt</name></author><entry><title>Solvers for Portfolio Optimization of Exchange Traded Funds</title><link href="http://blog.tomborgstadt.com//blog/2016/07/python-solver-packages" rel="alternate" type="text/html" title="Solvers for Portfolio Optimization of Exchange Traded Funds" /><published>2016-07-20T00:00:00-05:00</published><updated>2016-07-20T00:00:00-05:00</updated><id>http://blog.tomborgstadt.com//blog/2016/07/python-solver-packages</id><content type="html" xml:base="http://blog.tomborgstadt.com//blog/2016/07/python-solver-packages">&lt;p&gt;This project explores two Python packages for solving quadratic equations using Markowitz portfolio optimization as a metaphor. The popular Microsoft Excel Solver add-in is used to compare and verify results. The long-term objective of this research and development is to progress a fully automated optimization tool to aid in managing asset position sizes in a long-term investment portfolio of ETFs. The benefit of such a tool takes the guesswork out of picking assets in which to invest by using a systematic approach using mathematics, statistics, and portfolio theory to make decisions.&lt;/p&gt;

&lt;p&gt;This project is the beginning of longer term project for me. The objective here was to simply do some preliminary investigation of solver type packages that could be used to optimize non-linear quadratic problems with Python.&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#data-description&quot; id=&quot;markdown-toc-data-description&quot;&gt;Data Description&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#exploratory-data-analysis&quot; id=&quot;markdown-toc-exploratory-data-analysis&quot;&gt;Exploratory Data Analysis&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#process-and-results&quot; id=&quot;markdown-toc-process-and-results&quot;&gt;Process and Results&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conclusion&quot; id=&quot;markdown-toc-conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#notebook&quot; id=&quot;markdown-toc-notebook&quot;&gt;Notebook&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot; id=&quot;markdown-toc-references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;data-description&quot;&gt;Data Description&lt;/h3&gt;

&lt;p&gt;Historical ETF market data from Yahoo Finance is used in this analysis. The advantage of using ETFs versus stocks is that each ETF is a basket of assets (like mutual funds) and therefore is diversified. However, unlike mutual funds ETFs trade like stocks and have much lower expenses. Another powerful advantage of using ETFs is the number available and the diversity of ETFs that can be found in any area of interest whether an index, sector, particular industry, commodity, fixed, or currency. If the reader prefers investing in stocks only, the work completed here still applies - simply use the ticker symbols for your desired stocks.&lt;/p&gt;

&lt;p&gt;The following ETFs are used in this analysis:&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Ticker&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Name&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Category&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SPY&lt;/td&gt;
      &lt;td&gt;SPDR S&amp;amp;P 500 ETF Trust&lt;/td&gt;
      &lt;td&gt;Equities - Large Cap&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SH&lt;/td&gt;
      &lt;td&gt;ProShares Short S&amp;amp;P500 ETF&lt;/td&gt;
      &lt;td&gt;Equities - Large Cap&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;MDY&lt;/td&gt;
      &lt;td&gt;SPDR S&amp;amp;P MidCap 400 ETF&lt;/td&gt;
      &lt;td&gt;Equities - Mid Cap&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;VB&lt;/td&gt;
      &lt;td&gt;Vanguard Small-Cap Index Fund&lt;/td&gt;
      &lt;td&gt;Equities - Small Cap&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;VEU&lt;/td&gt;
      &lt;td&gt;Vanguard FTSE All-World ex-US ETF&lt;/td&gt;
      &lt;td&gt;Equities - Global&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;AGG&lt;/td&gt;
      &lt;td&gt;iShares Barclays Aggregate Bond Fund&lt;/td&gt;
      &lt;td&gt;Fixed Income&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;TLH&lt;/td&gt;
      &lt;td&gt;iShares Lehman 10-20 Yr Treasury Bond ETF&lt;/td&gt;
      &lt;td&gt;Fixed Income&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;RWR&lt;/td&gt;
      &lt;td&gt;SPDR Dow Jones REIT ETF&lt;/td&gt;
      &lt;td&gt;Real Estate&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;GLD&lt;/td&gt;
      &lt;td&gt;SPDR Gold Trust ETF&lt;/td&gt;
      &lt;td&gt;Commodity&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Daily market adjusted closing price from July 1, 2015, to July 1, 2016, is retrieved from Yahoo Finance for each ETF. The Python pandas package is utilized to generate returns. Returns are calculated based on the last day of the business month, so we have a 12 period series for each ETF.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Returns&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;SPY&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;SH&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;MDY&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;VB&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;VEU&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;AGG&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;TLH&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;RWR&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;GLD&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;8/31/2015&lt;/td&gt;
      &lt;td&gt;-0.061&lt;/td&gt;
      &lt;td&gt;0.0591&lt;/td&gt;
      &lt;td&gt;-0.0566&lt;/td&gt;
      &lt;td&gt;-0.0583&lt;/td&gt;
      &lt;td&gt;-0.0775&lt;/td&gt;
      &lt;td&gt;-0.0034&lt;/td&gt;
      &lt;td&gt;-0.0054&lt;/td&gt;
      &lt;td&gt;-0.0591&lt;/td&gt;
      &lt;td&gt;0.0371&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;9/30/2015&lt;/td&gt;
      &lt;td&gt;-0.0255&lt;/td&gt;
      &lt;td&gt;0.0209&lt;/td&gt;
      &lt;td&gt;-0.032&lt;/td&gt;
      &lt;td&gt;-0.0454&lt;/td&gt;
      &lt;td&gt;-0.0403&lt;/td&gt;
      &lt;td&gt;0.0081&lt;/td&gt;
      &lt;td&gt;0.0183&lt;/td&gt;
      &lt;td&gt;0.0339&lt;/td&gt;
      &lt;td&gt;-0.018&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;10/30/2015&lt;/td&gt;
      &lt;td&gt;0.0851&lt;/td&gt;
      &lt;td&gt;-0.0804&lt;/td&gt;
      &lt;td&gt;0.0555&lt;/td&gt;
      &lt;td&gt;0.0568&lt;/td&gt;
      &lt;td&gt;0.0669&lt;/td&gt;
      &lt;td&gt;0.0007&lt;/td&gt;
      &lt;td&gt;-0.0078&lt;/td&gt;
      &lt;td&gt;0.0579&lt;/td&gt;
      &lt;td&gt;0.0228&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;11/30/2015&lt;/td&gt;
      &lt;td&gt;0.0037&lt;/td&gt;
      &lt;td&gt;-0.0053&lt;/td&gt;
      &lt;td&gt;0.0134&lt;/td&gt;
      &lt;td&gt;0.018&lt;/td&gt;
      &lt;td&gt;-0.0132&lt;/td&gt;
      &lt;td&gt;-0.0039&lt;/td&gt;
      &lt;td&gt;-0.004&lt;/td&gt;
      &lt;td&gt;-0.0061&lt;/td&gt;
      &lt;td&gt;-0.0675&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;12/31/2015&lt;/td&gt;
      &lt;td&gt;-0.0173&lt;/td&gt;
      &lt;td&gt;0.0136&lt;/td&gt;
      &lt;td&gt;-0.042&lt;/td&gt;
      &lt;td&gt;-0.0419&lt;/td&gt;
      &lt;td&gt;-0.0252&lt;/td&gt;
      &lt;td&gt;-0.0019&lt;/td&gt;
      &lt;td&gt;-0.0036&lt;/td&gt;
      &lt;td&gt;0.022&lt;/td&gt;
      &lt;td&gt;-0.0045&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1/29/2016&lt;/td&gt;
      &lt;td&gt;-0.0498&lt;/td&gt;
      &lt;td&gt;0.0494&lt;/td&gt;
      &lt;td&gt;-0.0554&lt;/td&gt;
      &lt;td&gt;-0.0755&lt;/td&gt;
      &lt;td&gt;-0.056&lt;/td&gt;
      &lt;td&gt;0.0124&lt;/td&gt;
      &lt;td&gt;0.037&lt;/td&gt;
      &lt;td&gt;-0.0406&lt;/td&gt;
      &lt;td&gt;0.0541&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2/29/2016&lt;/td&gt;
      &lt;td&gt;-0.0008&lt;/td&gt;
      &lt;td&gt;-0.0023&lt;/td&gt;
      &lt;td&gt;0.0124&lt;/td&gt;
      &lt;td&gt;0.0077&lt;/td&gt;
      &lt;td&gt;-0.0244&lt;/td&gt;
      &lt;td&gt;0.0089&lt;/td&gt;
      &lt;td&gt;0.0186&lt;/td&gt;
      &lt;td&gt;-0.0085&lt;/td&gt;
      &lt;td&gt;0.1093&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3/31/2016&lt;/td&gt;
      &lt;td&gt;0.0673&lt;/td&gt;
      &lt;td&gt;-0.0659&lt;/td&gt;
      &lt;td&gt;0.0848&lt;/td&gt;
      &lt;td&gt;0.0846&lt;/td&gt;
      &lt;td&gt;0.083&lt;/td&gt;
      &lt;td&gt;0.0087&lt;/td&gt;
      &lt;td&gt;0.0013&lt;/td&gt;
      &lt;td&gt;0.1036&lt;/td&gt;
      &lt;td&gt;-0.0084&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4/29/2016&lt;/td&gt;
      &lt;td&gt;0.0039&lt;/td&gt;
      &lt;td&gt;-0.0049&lt;/td&gt;
      &lt;td&gt;0.0119&lt;/td&gt;
      &lt;td&gt;0.0177&lt;/td&gt;
      &lt;td&gt;0.0211&lt;/td&gt;
      &lt;td&gt;0.0026&lt;/td&gt;
      &lt;td&gt;-0.0038&lt;/td&gt;
      &lt;td&gt;-0.0296&lt;/td&gt;
      &lt;td&gt;0.0511&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5/31/2016&lt;/td&gt;
      &lt;td&gt;0.017&lt;/td&gt;
      &lt;td&gt;-0.0187&lt;/td&gt;
      &lt;td&gt;0.0225&lt;/td&gt;
      &lt;td&gt;0.0189&lt;/td&gt;
      &lt;td&gt;-0.0079&lt;/td&gt;
      &lt;td&gt;0.0001&lt;/td&gt;
      &lt;td&gt;0.0029&lt;/td&gt;
      &lt;td&gt;0.0198&lt;/td&gt;
      &lt;td&gt;-0.0614&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;6/30/2016&lt;/td&gt;
      &lt;td&gt;0.0035&lt;/td&gt;
      &lt;td&gt;-0.005&lt;/td&gt;
      &lt;td&gt;0.0047&lt;/td&gt;
      &lt;td&gt;0.0033&lt;/td&gt;
      &lt;td&gt;-0.0073&lt;/td&gt;
      &lt;td&gt;0.0194&lt;/td&gt;
      &lt;td&gt;0.0418&lt;/td&gt;
      &lt;td&gt;0.0646&lt;/td&gt;
      &lt;td&gt;0.0897&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;7/29/2016&lt;/td&gt;
      &lt;td&gt;0.0021&lt;/td&gt;
      &lt;td&gt;-0.0023&lt;/td&gt;
      &lt;td&gt;0.002&lt;/td&gt;
      &lt;td&gt;0.003&lt;/td&gt;
      &lt;td&gt;0.003&lt;/td&gt;
      &lt;td&gt;0.0022&lt;/td&gt;
      &lt;td&gt;0.0071&lt;/td&gt;
      &lt;td&gt;0.0013&lt;/td&gt;
      &lt;td&gt;0.0153&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ER&lt;/td&gt;
      &lt;td&gt;0.0023&lt;/td&gt;
      &lt;td&gt;-0.0035&lt;/td&gt;
      &lt;td&gt;0.0018&lt;/td&gt;
      &lt;td&gt;-0.0009&lt;/td&gt;
      &lt;td&gt;-0.0065&lt;/td&gt;
      &lt;td&gt;0.0045&lt;/td&gt;
      &lt;td&gt;0.0085&lt;/td&gt;
      &lt;td&gt;0.0133&lt;/td&gt;
      &lt;td&gt;0.0183&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SD&lt;/td&gt;
      &lt;td&gt;0.0417&lt;/td&gt;
      &lt;td&gt;0.0401&lt;/td&gt;
      &lt;td&gt;0.043&lt;/td&gt;
      &lt;td&gt;0.0471&lt;/td&gt;
      &lt;td&gt;0.0463&lt;/td&gt;
      &lt;td&gt;0.0071&lt;/td&gt;
      &lt;td&gt;0.0168&lt;/td&gt;
      &lt;td&gt;0.047&lt;/td&gt;
      &lt;td&gt;0.0543&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;exploratory-data-analysis&quot;&gt;Exploratory Data Analysis&lt;/h3&gt;

&lt;p&gt;For purposes of general evaluation of the packages involved, it is assumed that Yahoo Finance data is relatively clean and accurate, so minimal EDA was performed. However, a correlation matrix was created and inspected to ensure the returns calculated were reasonable. For example, Treasury bills (TLH) are inversely correlated with the S&amp;amp;P 500 (SPY).&lt;/p&gt;

&lt;figure&gt;&lt;a&gt;&lt;img src=&quot;/images/20160720_corr.png&quot; /&gt;&lt;/a&gt;
&lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;process-and-results&quot;&gt;Process and Results&lt;/h3&gt;
&lt;p&gt;First, the portfolio frontier was simulated and visualized by generating 1500 different portfolios by randomly generating asset weights and calculating the portfolio expected return and risk for the given ETFs.&lt;/p&gt;

&lt;figure&gt;&lt;a&gt;&lt;img src=&quot;/images/20160720_cvxopt_ef1.png&quot; /&gt;&lt;/a&gt;
&lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Once the portfolio frontier was created, the efficient frontier could be added by fitting a 2nd-degree polynomial to the data and generating expected returns along the curve.&lt;/p&gt;

&lt;figure&gt;&lt;a&gt;&lt;img src=&quot;/images/20160720_cvxopt_ef2.png&quot; /&gt;&lt;/a&gt;
&lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Portfolios with fewer assets, or ETFs, generated more visually appealing frontiers. For example, the following visualization was generated using only 3 ETFs.  As the portfolio increases in the number of assets, the portfolio frontier seems to become less well defined which deserves additional research as to why this might be.&lt;/p&gt;

&lt;figure&gt;&lt;a&gt;&lt;img src=&quot;/images/20160720_2cvxopt_ef2.png&quot; /&gt;&lt;/a&gt;
&lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Two Python solver packages were installed and implemented to evaluate performance and accuracy compared to results provided by the Excel Solver add-in for the same portfolio of ETFs. Both the Python packages are open source and free.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://cvxopt.org&quot; target=&quot;blank&quot;&gt;CVXOPT&lt;/a&gt; was developed at MIT by J.Dahl, M.Anderson, and L. Vandenberghe beginning in 2004. The latest version was released in 2015.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.cvxpy.org/&quot; target=&quot;blank&quot;&gt;CVXPY&lt;/a&gt; &lt;a href=&quot;#cvxpy&quot;&gt;(Diamond &amp;amp; Boyd, 2016)&lt;/a&gt; was developed by Steven Diamond and Stephen Boyd. It was published in the Journal of Machine Learning Research. And actually it is not a solver in and of itself but rather transforms a problem into a standard form before calling a solver and ultimately deliveres the results from the solver in a standard form.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All three solvers provided identical results. CVXOPT has a sophisticated constraint matrices setup and should be able to handle most any constraint combination. This analysis was minimal and more time will be spent in future work to explore and identify any constraint limitations.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Objective 1:&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Maximize Portfolio Return (ERP)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Constraints:&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Sum(Weights) = 1; At least 5% investment in each ETF&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Excel Solver&lt;/td&gt;
      &lt;td&gt;CVXOPT&lt;/td&gt;
      &lt;td&gt;CVXPY&lt;/td&gt;
      &lt;td&gt;Weights&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ER&lt;/td&gt;
      &lt;td&gt;0.011955&lt;/td&gt;
      &lt;td&gt;0.011955&lt;/td&gt;
      &lt;td&gt;0.011955&lt;/td&gt;
      &lt;td&gt;60% GLD, 5% all others&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SD&lt;/td&gt;
      &lt;td&gt;0.032858&lt;/td&gt;
      &lt;td&gt;0.032858&lt;/td&gt;
      &lt;td&gt;0.032858&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Objective 2:&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Minimize Portfolio Risk (SDP)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Constraints:&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Sum(Weights) = 1; At least 5% investment in each ETF&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Excel Solver&lt;/td&gt;
      &lt;td&gt;CVXOPT&lt;/td&gt;
      &lt;td&gt;CVXPY&lt;/td&gt;
      &lt;td&gt;Weights&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ER&lt;/td&gt;
      &lt;td&gt;0.001003&lt;/td&gt;
      &lt;td&gt;na&lt;/td&gt;
      &lt;td&gt;0.001003&lt;/td&gt;
      &lt;td&gt;60% GLD, 5% all others&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SD&lt;/td&gt;
      &lt;td&gt;0.004296&lt;/td&gt;
      &lt;td&gt;na&lt;/td&gt;
      &lt;td&gt;0.004296&lt;/td&gt;
      &lt;td&gt;23% SPY, 42% SH, 5% All Others&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;The two Python based solvers evaluated in this very &lt;em&gt;limited&lt;/em&gt; analysis performed as expected. CVXOPT is a mature package but is equally more sophisticated, whereas CVXPY is more intuitive when it comes to setting up and defining an optimization problem in Python. In other words, it abstracts the extensive matrices mathematics knowledge required by CVXOPT to configure problem constraints. For this reason, CVXPY has a much lower learning curve. Both solvers provided identical results when compared to the Excel Solver add-on.&lt;/p&gt;

&lt;p&gt;This project resulted in an automated tool (see included Python HTML notebook) for compiling returns and generating differently weighted portfolios for a set of assets, namely ETFs or stocks. This is the first piece of a larger project to develop a method for rebalancing a long-term investment portfolio that I am progressing and will post additional followup.&lt;/p&gt;

&lt;p&gt;Future objectives given this fundamental groundwork include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Measure impact of time period for returns based on rebalancing frequency&lt;/li&gt;
  &lt;li&gt;Tracking changes in optimal portfolio weights to predict future weights&lt;/li&gt;
  &lt;li&gt;Capability to backtest&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;notebook&quot;&gt;Notebook&lt;/h3&gt;

&lt;p&gt;Jupyter Notebook: &lt;a href=&quot;/notebooks/20160720-portfolio-optimization-solvers.html&quot; target=&quot;_blank&quot;&gt;Portfolio Optimization Solvers&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;p&gt;Starke, Thomas, Ph.D. “The Efficient Frontier: Markowitz Portfolio Optimization in Python.” Quantopian. N.p., 9 Mar. 2015. Web. 15 July 2016.&lt;/p&gt;</content><author><name>Tom Borgstadt</name></author><category term="non-linear optimization" /><category term="cvxopt" /><category term="cvxpy" /><summary>This project explores two Python packages for solving quadratic equations using Markowitz portfolio optimization as a metaphor. The popular Microsoft Excel Solver add-in is used to compare and verify results. The long-term objective of this research and development is to progress a fully automated optimization tool to aid in managing asset position sizes in a long-term investment portfolio of ETFs. The benefit of such a tool takes the guesswork out of picking assets in which to invest by using a systematic approach using mathematics, statistics, and portfolio theory to make decisions.</summary></entry><entry><title>Tune $50K Salary Estimators using Spark and Explore MLlib</title><link href="http://blog.tomborgstadt.com//blog/2016/03/spark-dive" rel="alternate" type="text/html" title="Tune $50K Salary Estimators using Spark and Explore MLlib" /><published>2016-03-23T00:00:00-05:00</published><updated>2016-03-23T00:00:00-05:00</updated><id>http://blog.tomborgstadt.com//blog/2016/03/spark-dive</id><content type="html" xml:base="http://blog.tomborgstadt.com//blog/2016/03/spark-dive">&lt;p&gt;This is purely an academic project to begin exploring Spark.&lt;/p&gt;

&lt;p&gt;The objectives of the project:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Configure and use Jupyter Notebook with Spark&lt;/li&gt;
  &lt;li&gt;Use &lt;a href=&quot;https://databricks.com/&quot; target=&quot;_blank&quot;&gt;databricks&lt;/a&gt; spark-sklearn package to boost SciKit Learn’s grid search&lt;/li&gt;
  &lt;li&gt;Implement &lt;a href=&quot;http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#module-pyspark.mllib.tree&quot; target=&quot;_blank&quot;&gt;Spark MLlib tree module&lt;/a&gt; classifiers&lt;/li&gt;
&lt;/ol&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#environment&quot; id=&quot;markdown-toc-environment&quot;&gt;Environment&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#previous-work&quot; id=&quot;markdown-toc-previous-work&quot;&gt;Previous Work&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#data-set&quot; id=&quot;markdown-toc-data-set&quot;&gt;Data Set&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#recommended-estimator&quot; id=&quot;markdown-toc-recommended-estimator&quot;&gt;Recommended Estimator&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#experiments-and-results&quot; id=&quot;markdown-toc-experiments-and-results&quot;&gt;Experiments and Results&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#notebook&quot; id=&quot;markdown-toc-notebook&quot;&gt;Notebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;environment&quot;&gt;Environment&lt;/h3&gt;

&lt;p&gt;Although I’m familiar with installing a Spark cluster, I opted to use a &lt;a href=&quot;https://www.docker.com/&quot; target=&quot;_blank&quot;&gt;Docker&lt;/a&gt; stack &lt;a href=&quot;https://github.com/jupyter/docker-stacks/tree/master/all-spark-notebook&quot; target=&quot;_blank&quot;&gt;all-spark-notebook&lt;/a&gt; made available by the Jupyter project for my local environment. This was my first time using &lt;a href=&quot;https://www.docker.com/&quot; target=&quot;_blank&quot;&gt;Docker&lt;/a&gt;, and I must admit it is an impressive tool. I will be looking at it further.&lt;/p&gt;

&lt;p&gt;For my Amazon environment, I opted for an EMR 5 node cluster with “all” apache products installed. I then installed anaconda, configured Jupyter, and installed the DataBricks package manually.&lt;/p&gt;

&lt;p&gt;I did have a problem with the databricks spark-sklearn package in the AWS environment. This needs additional troubleshooting so I have included only the Jupyter notebook from my local Docker environment at the end.&lt;/p&gt;

&lt;h3 id=&quot;previous-work&quot;&gt;Previous Work&lt;/h3&gt;

&lt;p&gt;In a previous project I evaluated performance of a number of different SciKit Learn classifiers on the following data set. The top two classifiers from that project will be used for basis of this project.&lt;/p&gt;

&lt;h4 id=&quot;data-set&quot;&gt;Data Set&lt;/h4&gt;

&lt;p&gt;The data set was the Adult Census dataset from the &lt;a href=&quot;https://archive.ics.uci.edu/ml/datasets/Adult&quot; target=&quot;blank&quot;&gt;UCI Machine Learning Repository&lt;/a&gt;. My objective is to predict the minority class, or those people that make &lt;strong&gt;more than $50K&lt;/strong&gt; annually based on a number of different demographic features from the census.&lt;/p&gt;

&lt;p&gt;The dataset is imbalanced 3 to 1 in favor of the majority class, or those that &lt;strong&gt;make $50K or less&lt;/strong&gt;.&lt;/p&gt;

&lt;h4 id=&quot;recommended-estimator&quot;&gt;Recommended Estimator&lt;/h4&gt;

&lt;p&gt;A Bagged Linear Support Vector was recommended because it provided the best predictability of the minority class as measured by recall. A Gradient Boosting Classifier was the second best but actually quite poor predictor of the minority class however, it is included here to see if it can be tuned further.&lt;/p&gt;

&lt;h3 id=&quot;experiments-and-results&quot;&gt;Experiments and Results&lt;/h3&gt;

&lt;p&gt;For the experiments I performed, the Linear SVC and Gradient Boosting Classifer were extensively grid searched using the spark-sklearn package from &lt;a href=&quot;https://databricks.com/&quot; target=&quot;_blank&quot;&gt;databricks&lt;/a&gt;.&lt;/p&gt;

&lt;figure&gt;&lt;a&gt;&lt;img src=&quot;/images/sparkdive1.jpg&quot; /&gt;&lt;/a&gt;
&lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;First, a small improvement of the Bagged Linear Support Vector from the previous was made. This was accomplished with a small increase in the linear intercept that the additional grid search was able to find.&lt;/p&gt;

&lt;p&gt;Second, I incorporated two conceptual feature selection algorithms for consideration. The first takes the top 60% of the most important features as identified by a random forest. The second, uses a K­means clustering classifier with 14 clusters in this case, and takes the top 5 features from each cluster to identify the top features. I did not get to bag these estimators over the smaller feature list, however, I suspect there are a number of correlated features considering the performance of these estimators is only slightly less than the boosted - both evident with the Linear Support Vector and Gradient Boost Classifiers.&lt;/p&gt;

&lt;p&gt;I tried a manifold approach to feature selection by using SciKit Learn’s Locally Linear Embedding (LLE) model but the module has a memory leak once several hundred observations are encountered. The code is included in the notebook for reference. Apparently the bug is a known issue.&lt;/p&gt;

&lt;p&gt;Lastly, the &lt;a href=&quot;http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#module-pyspark.mllib.tree&quot; target=&quot;_blank&quot;&gt;Spark MLlib tree module&lt;/a&gt; classifiers are included. They performed quite well although none were tuned - all results reflect default parameters. The Random Forest did quite well and I’m anxious to spend some time tuning it further.&lt;/p&gt;

&lt;h3 id=&quot;notebook&quot;&gt;Notebook&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/notebooks/Spark-Dive.html&quot; target=&quot;_blank&quot;&gt;Exploring PySpark Notebook&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That about wraps it up.&lt;/p&gt;</content><author><name>Tom Borgstadt</name></author><category term="classification" /><category term="docker" /><category term="mllib" /><category term="pyspark" /><category term="scikit-learn" /><category term="spark-sklearn" /><summary>This is purely an academic project to begin exploring Spark.</summary></entry><entry><title>Predicting Salaries Greater than $50K</title><link href="http://blog.tomborgstadt.com//blog/2016/02/classification" rel="alternate" type="text/html" title="Predicting Salaries Greater than $50K" /><published>2016-02-17T00:00:00-06:00</published><updated>2016-02-17T00:00:00-06:00</updated><id>http://blog.tomborgstadt.com//blog/2016/02/classification</id><content type="html" xml:base="http://blog.tomborgstadt.com//blog/2016/02/classification">&lt;p&gt;This is an academic exercise in applying and tuning numerous classification models in order to predict individual salaries greater than $50K given demographics from a census dataset. We will consider classifiers from scikit-learn, pybrain, and theanets.&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#data-set&quot; id=&quot;markdown-toc-data-set&quot;&gt;Data Set&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#environments&quot; id=&quot;markdown-toc-environments&quot;&gt;Environments&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#experiment-process&quot; id=&quot;markdown-toc-experiment-process&quot;&gt;Experiment Process&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#scikit-learn-results&quot; id=&quot;markdown-toc-scikit-learn-results&quot;&gt;Scikit-Learn Results&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#neural-nets-results&quot; id=&quot;markdown-toc-neural-nets-results&quot;&gt;Neural Nets Results&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#the-challenge-and-conclusion&quot; id=&quot;markdown-toc-the-challenge-and-conclusion&quot;&gt;The Challenge and Conclusion&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#notebooks&quot; id=&quot;markdown-toc-notebooks&quot;&gt;Notebooks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;data-set&quot;&gt;Data Set&lt;/h4&gt;

&lt;p&gt;The &lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/Adult&quot; target=&quot;_blank&quot;&gt;Adult Census&lt;/a&gt; dataset from UCI Machine Learning Repository was donated by Ronny Kohavi and Barry Becker of Data Mining and Visualization Silicon Graphics. The data consists of various demographic information collected by the US Census. The classification task is to predict whether an individual makes over $50K per year income.&lt;/p&gt;

&lt;p&gt;The dataset is provided in two sets, one for training and one for testing. For this project, the two sets were put back together for preprocessing and EDA before applying a 60/40 training/testing split.&lt;/p&gt;

&lt;p&gt;After preprocessing the total number of records was 48,841. The feature set grew from 15 to 109 after categorical features were converted to binary (True/False, 1/0). The dataset was scaled before split into training and testing sets.&lt;/p&gt;

&lt;p&gt;This exercise is a two class classification, meaning either the person makes more than $50K per year, or they make $50K or less. The class distribution of the dataset is the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;24% - make greater than $50K  (value=1, class of interest is the minority class)&lt;/li&gt;
  &lt;li&gt;76% - make $50K or less       (value=0)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The dataset is imbalanced meaning there is significant difference between number of people making more than $50K and those that do not. Furthermore, the class of interest is the minority class which presents challenges in optimizing prediction of people making more than $50K.&lt;/p&gt;

&lt;h4 id=&quot;environments&quot;&gt;Environments&lt;/h4&gt;

&lt;p&gt;Two environments were utilized for experiements. The first environment was standard Anaconda running on Ubuntu 14.04. The Anaconda environment was used primarily for scikit-learn classifiers.&lt;/p&gt;

&lt;p&gt;The second environment was installed using a Docker image and was utilized for the neural networks portion of the exercise. The Docker image consists of the &lt;a href=&quot;http://yandex.github.io/rep/index.html#&quot; target=&quot;_blank&quot;&gt;The Reproducible Experiment Platform (REP)&lt;/a&gt;. I used the estimators module which contains wrappers that follow scikit-learn interface style for three neural network machine learning libraries - pybrain, neurolab, and theanets. The REP environment provides interoperatbilty of these neural network classifiers with scikit-learn boosting algorithms. For example, it is very easy to boost a pybrain classifier with scikit-learn adaboost algorithm.&lt;/p&gt;

&lt;h4 id=&quot;experiment-process&quot;&gt;Experiment Process&lt;/h4&gt;

&lt;p&gt;The process for tuning classifiers was iterative. Each classifier was run with default parameter values as a baseline and then tuned with grid search until the best estimator for each was identified. The result of each experiment (including grid search parms) was recorded in a csv file for reference and analysis for tuning the next step.&lt;/p&gt;

&lt;p&gt;The following classification algorithms were used. Note number of experiments for each and total build time was captured.&lt;/p&gt;

&lt;figure&gt;&lt;a&gt;&lt;img src=&quot;/images/class1.png&quot; /&gt;&lt;/a&gt;
&lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;scikit-learn-results&quot;&gt;Scikit-Learn Results&lt;/h4&gt;

&lt;p&gt;Below is ROC curves for top 5 performing estimators in the Anaconda environment. Based on ROC alone, Gradient Boost and Bagged Random Forest outperform the others.&lt;/p&gt;

&lt;figure&gt;&lt;a&gt;&lt;img src=&quot;/images/class2.png&quot; /&gt;&lt;/a&gt;
&lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;neural-nets-results&quot;&gt;Neural Nets Results&lt;/h4&gt;

&lt;p&gt;Below are ROC curves for experiments conducted in the REP environment. Gradient Boost was run in this environment to compare and agree with the same in the Anaconda environment. PyBrain and Theanet neural networks deserve addtional tuning with various network configurations, but clearly we can see differences in tuning parameters. For example, the Theanet­30­30 is a two hidden layer network with 30 neurons in each hidden layer. It has default learning parameter of .1. By comparison, the Theanet­30­30­.2 is the same network with .2 learning parm.&lt;/p&gt;

&lt;p&gt;Note the &lt;strong&gt;Boosted Theanet-30-30-.5&lt;/strong&gt; is example of how REP classes can accommodate boosting the Theanet classifier with scikit-learn AdaBoost boosting algorithm.&lt;/p&gt;

&lt;figure&gt;&lt;a&gt;&lt;img src=&quot;/images/class3.png&quot; /&gt;&lt;/a&gt;
&lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;the-challenge-and-conclusion&quot;&gt;The Challenge and Conclusion&lt;/h4&gt;

&lt;p&gt;The challenge with this dataset is the fact that is imbalanced. The overall accuracy of the model is of secondary importance as we desire to predict the minority class. In other words, optimizing recall to improve predictibility of the minority class should be our objective.&lt;/p&gt;

&lt;p&gt;The Bagged Linear SVC is able to best predict the minority class (those that make $50K or more) with 85 percent recall. It also has a good ROC score. The neural nets performed next best but were actually quite poor in predicting the minority class. The discrepency of the recall versus the cross validation of recall on the neural nets is an additional concern.&lt;/p&gt;

&lt;figure&gt;&lt;a&gt;&lt;img src=&quot;/images/class4.png&quot; /&gt;&lt;/a&gt;
&lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;ROC Score Key:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;.90­ to 1.0 = excellent (A)&lt;/li&gt;
  &lt;li&gt;.80­ to .90 = good (B)&lt;/li&gt;
  &lt;li&gt;.70­ to .80 = fair (C)&lt;/li&gt;
  &lt;li&gt;.60­ to .70 = poor (D)&lt;/li&gt;
  &lt;li&gt;.50­ to .60 = fail (F)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A Bagged Linear Support Vector is recommended because it provides the best predictability of the minority class as measured by recall.&lt;/p&gt;

&lt;h4 id=&quot;notebooks&quot;&gt;Notebooks&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/notebooks/AdultCensus-Sklearn.html&quot; target=&quot;_blank&quot;&gt;Adult Census Scikit Learn Experiments&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/notebooks/AdultCensus-NN.html&quot; target=&quot;_blank&quot;&gt;Adult Census Neural Net Experiments&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Tom Borgstadt</name></author><category term="classification" /><category term="docker" /><category term="neural networks" /><category term="pybrain" /><category term="theanets" /><category term="rep" /><category term="scikit-learn" /><summary>This is an academic exercise in applying and tuning numerous classification models in order to predict individual salaries greater than $50K given demographics from a census dataset. We will consider classifiers from scikit-learn, pybrain, and theanets.</summary></entry><entry><title>VirtualBox 4.3 Shared Folders Setup</title><link href="http://blog.tomborgstadt.com//blog/2016/01/virtualbox" rel="alternate" type="text/html" title="VirtualBox 4.3 Shared Folders Setup" /><published>2016-01-10T00:00:00-06:00</published><updated>2016-01-10T00:00:00-06:00</updated><id>http://blog.tomborgstadt.com//blog/2016/01/virtualbox</id><content type="html" xml:base="http://blog.tomborgstadt.com//blog/2016/01/virtualbox">&lt;p&gt;Virtualbox is a valuable tool for prototyping new applications or testing unfamiliar applications without risk to your production host. I use it for these reasons but also to run different operating systems when needed. For example, I have a Windows guest for Microsoft applications that I use. I also use guest machines to stand up development database servers. For example, I have a postgreSQL guest and a MySQL guest.&lt;/p&gt;

&lt;p&gt;This post explains how to share a host file folder with a VirtualBox guest. I have also included my experience using insync and Google Drive.&lt;/p&gt;

&lt;p&gt;Objectives&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Synchronize select host folders with Google Drive for backup and accessiblity from the web&lt;/li&gt;
  &lt;li&gt;Access selected host folders from a VirtualBox guest(s) as needed&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;synchronizing-google-drive&quot;&gt;Synchronizing Google Drive&lt;/h3&gt;
&lt;p&gt;I use &lt;a href=&quot;https://www.insynchq.com/&quot; target=&quot;_blank&quot;&gt;insync&lt;/a&gt; to synchronize folders from Google Drive to my local Ubuntu host. Insync is available for Windows, Mac, and Linux. My experience with insync is very positive. The key features I like include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;sychronize Google Drive folders from up to 3 separate Google Accounts&lt;/li&gt;
  &lt;li&gt;ability to select exactly which Google Drive folders you want to synchronize&lt;/li&gt;
  &lt;li&gt;it runs on Linux&lt;/li&gt;
  &lt;li&gt;insync is customer focused, interested in feedback and suggestions for their product, not too much but in touch and responsive which is nice&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;configuring-virtualbox-shared-folders&quot;&gt;Configuring VirtualBox Shared Folders&lt;/h3&gt;

&lt;p&gt;The following can be done to share any host file folder with a guest. I have shared my synchronized Google Drive folder here only for example.&lt;/p&gt;

&lt;h4 id=&quot;from-the-virtualbox-host&quot;&gt;From the VirtualBox host:&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Open VirtualBox Manager&lt;/li&gt;
  &lt;li&gt;Select the guest needing access to a host folder (e.g. ‘postgre’)&lt;/li&gt;
  &lt;li&gt;Under settings select ‘Shared Folders’&lt;/li&gt;
  &lt;li&gt;Click on ‘Machine Folders’ and click ‘Add new shared folder definition’&lt;/li&gt;
  &lt;li&gt;Browse to the local path of the desired folder&lt;/li&gt;
  &lt;li&gt;Select ‘Auto-mount’ and ‘Make Permanent’&lt;/li&gt;
  &lt;li&gt;Click ‘OK’ and ‘OK’&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;&lt;a&gt;&lt;img src=&quot;/images/vbxshrdfldr1.png&quot; /&gt;&lt;/a&gt;
&lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;from-a-virtualbox-guest&quot;&gt;From a VirtualBox guest:&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Login&lt;/li&gt;
  &lt;li&gt;Add your user to the vbox group&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;gp&quot;&gt;tom@postgre:~$ &lt;/span&gt;sudo usermod -aG vboxsf tom &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Logout and log back in&lt;/li&gt;
  &lt;li&gt;Browse to the system ‘/media’ folder and verify the mapped folder is present. By default VirtualBox will mount the shared folder in the ‘/media’ folder with prefix ‘sf_’. We can see it below both from Nautilus and the terminal.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;&lt;a&gt;&lt;img src=&quot;/images/vbxshrdfldr2.png&quot; /&gt;&lt;/a&gt;
&lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;change-the-guest-mount-directory&quot;&gt;Change the guest mount directory:&lt;/h4&gt;

&lt;p&gt;I prefer to have the shared folder accessbile from my home directory (instead of folder ‘/media’) and this is easily achieved by setting the VirtualBox shared folders mount directory option for the guest using the command line on the host.&lt;/p&gt;

&lt;p&gt;From the host:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# Note this is set per guest system, it is not a global setting&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;tom@host:~$ &lt;/span&gt;vboxmanage guestproperty &lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;postgre&quot;&lt;/span&gt; /VirtualBox/GuestAdd/SharedFolders/MountDir /home/tom&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Restart the guest and now the shared folder is available from the home location.&lt;/p&gt;

&lt;figure&gt;&lt;a&gt;&lt;img src=&quot;/images/vbxshrdfldr3.png&quot; /&gt;&lt;/a&gt;
&lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;There you have it.&lt;/p&gt;</content><author><name>Tom Borgstadt</name></author><category term="virtualbox" /><category term="google drive" /><category term="ubuntu" /><summary>Virtualbox is a valuable tool for prototyping new applications or testing unfamiliar applications without risk to your production host. I use it for these reasons but also to run different operating systems when needed. For example, I have a Windows guest for Microsoft applications that I use. I also use guest machines to stand up development database servers. For example, I have a postgreSQL guest and a MySQL guest.</summary></entry><entry><title>NMF Topic Visualization with pyLDAvis (VW Scandal)</title><link href="http://blog.tomborgstadt.com//blog/2015/12/nmf-pyldavis" rel="alternate" type="text/html" title="NMF Topic Visualization with pyLDAvis (VW Scandal)" /><published>2015-12-18T00:00:00-06:00</published><updated>2015-12-18T00:00:00-06:00</updated><id>http://blog.tomborgstadt.com//blog/2015/12/nmf-pyldavis</id><content type="html" xml:base="http://blog.tomborgstadt.com//blog/2015/12/nmf-pyldavis">&lt;p&gt;Recently I completed a project to extract topics from articles related to the Volkswagen Diesel Scandal. The project utilized Graphlab to model topics and &lt;a href=&quot;https://pyldavis.readthedocs.org/en/latest/&quot; target=&quot;_blank&quot;&gt;pyLDAvis&lt;/a&gt; for visualization. As a follow on to the original project, I wanted to see if I could use pyLDAvis with non-negative matrix factorization (NMF) model for topic extraction.&lt;/p&gt;

&lt;p&gt;The following code snippet below assumes we have instantiated a vector over our corpus and fitted NMF model. See link to working visualization below.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;################################################&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Prepare data from NMF and Tfidf for pyLDAvis&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;################################################&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# topic_term_dists&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;topic_term_dists&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nmf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;components_&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# from NMF model&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# normalize topic term vectors required by pyLDAvis&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;topic_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;topic&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;topic_term_dists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;topic_term_dists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;topic_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;topic_term_dists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;topic_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;topic_term_dists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;topic_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;ord&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;topic_term_dists&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;topic_term_dists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;raise&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;ValueError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;Error - problem with topic term vector normalization&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# doc_topic_dists&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;doc_topic&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# iterate through all docs in tfidf vector&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;doc_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;doc&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v_model_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;doc_topic_sims&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    
    &lt;span class=&quot;c&quot;&gt;# iterate through all topics from NMF&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;topic_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;topic&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nmf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;components_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;  
        
        &lt;span class=&quot;c&quot;&gt;# calculate similarity of topic to document&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sim_value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cosine_similarity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;doc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;topic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;        
        &lt;span class=&quot;n&quot;&gt;doc_topic_sims&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sim_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;doc_topic&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;doc_topic_sims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
&lt;span class=&quot;n&quot;&gt;doc_topic_dists&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;doc_topic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# normalize topic features for each doc required by pyLDAvis&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;doc_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;doc&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;doc_topic_dists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;doc_topic_dists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;doc_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;doc_topic_dists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;doc_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;doc_topic_dists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;doc_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;ord&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;doc_topic_dists&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v_model_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;raise&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;ValueError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;Error - problem with document topic vector normalization&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# doc_lengths&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;doc_lengths&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v_model_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
&lt;span class=&quot;c&quot;&gt;# vocabulary&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;vocabulary&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v_model_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# term_frequency&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;term_frequency&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v_model_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;pyLDAvis_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;topic_term_dists&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;topic_term_dists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                 &lt;span class=&quot;s&quot;&gt;&#39;doc_topic_dists&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;doc_topic_dists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                 &lt;span class=&quot;s&quot;&gt;&#39;doc_lengths&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;      &lt;span class=&quot;n&quot;&gt;doc_lengths&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                 &lt;span class=&quot;s&quot;&gt;&#39;vocab&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;vocabulary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                 &lt;span class=&quot;s&quot;&gt;&#39;term_frequency&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;term_frequency&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pyLDAvis&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pyLDAvis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;enable_notebook&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;vis_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyLDAvis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prepare&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pyLDAvis_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;c&quot;&gt;# Display the interative visualization&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pyLDAvis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;display&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vis_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Resulting pyLDAvis topic visualization below. Find the entire working notebook here: &lt;a href=&quot;/notebooks/pyLDAvis-on-NMF.html#NMF-Topic-Visualization-with-pyLDAvis&quot; target=&quot;_blank&quot;&gt;pyLDAvis html interactive notebook&lt;/a&gt;.&lt;/p&gt;
&lt;figure&gt;&lt;a target=&quot;_blank&quot; href=&quot;/notebooks/pyLDAvis-on-NMF.html#NMF-Topic-Visualization-with-pyLDAvis&quot;&gt;&lt;img src=&quot;/images/pyldavis.png&quot; /&gt;&lt;/a&gt;
&lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Thanks to Ben Mabey for &lt;a href=&quot;http://nbviewer.ipython.org/github/bmabey/pyLDAvis/blob/master/notebooks/pyLDAvis_overview.ipynb&quot; target=&quot;_blank&quot;&gt;BYOM&lt;/a&gt; (bring your own model) information.&lt;/p&gt;</content><author><name>Tom Borgstadt</name></author><category term="nmf" /><category term="pyldavis" /><category term="topic extraction" /><category term="visualization" /><summary>Recently I completed a project to extract topics from articles related to the Volkswagen Diesel Scandal. The project utilized Graphlab to model topics and pyLDAvis for visualization. As a follow on to the original project, I wanted to see if I could use pyLDAvis with non-negative matrix factorization (NMF) model for topic extraction.</summary></entry><entry><title>Text Mining the Volkswagen Diesel Scandal</title><link href="http://blog.tomborgstadt.com//blog/2015/11/vw-scandal" rel="alternate" type="text/html" title="Text Mining the Volkswagen Diesel Scandal" /><published>2015-11-30T00:00:00-06:00</published><updated>2015-11-30T00:00:00-06:00</updated><id>http://blog.tomborgstadt.com//blog/2015/11/vw-scandal</id><content type="html" xml:base="http://blog.tomborgstadt.com//blog/2015/11/vw-scandal">&lt;p&gt;We own two Volkswagen diesel cars affected by the recent scandal where Volkswagen knowingly mislead the EPA and the public about emissions produced from their “clean diesel” technology. So, I was interested to learn about public sentiment and discussion topics during the 3 months since September 18th when the scandal broke.&lt;/p&gt;

&lt;p&gt;My intent was to use data from Twitter but the quote I received for the 3 months of data I needed was too much, so I decided instead to use articles published by &lt;a href=&quot;http://www.nytimes.com&quot; target=&quot;_blank&quot;&gt;The New York Times&lt;/a&gt; for my analysis.&lt;/p&gt;

&lt;p&gt;Clearly I found prominent topics throughout the 3 month period but this became more of a research project because I was unable to analyze sentiment and discussion from the public like I would have had with Twitter data. Nevertheless, it was an intersting exercise and I managed to explore a topic visualization package pyLDAvis as a bonus.&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#building-the-article-meta-data-list&quot; id=&quot;markdown-toc-building-the-article-meta-data-list&quot;&gt;Building the article meta data list&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#retrieving-new-york-times-article-text&quot; id=&quot;markdown-toc-retrieving-new-york-times-article-text&quot;&gt;Retrieving New York Times article text&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#retrieving-syndicated-article-text-from-thomson-reuters&quot; id=&quot;markdown-toc-retrieving-syndicated-article-text-from-thomson-reuters&quot;&gt;Retrieving syndicated article text from Thomson Reuters&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#topic-extraction&quot; id=&quot;markdown-toc-topic-extraction&quot;&gt;Topic extraction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#topic-visualization-with-phldavis&quot; id=&quot;markdown-toc-topic-visualization-with-phldavis&quot;&gt;Topic visualization with phLDAvis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;building-the-article-meta-data-list&quot;&gt;Building the article meta data list&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://developer.nytimes.com/docs/read/article_search_api_v2&quot; target=&quot;_blank&quot;&gt;The New York Times API&lt;/a&gt; was used to generate a list of articles. The following information or meta data for each article was collected using the API:&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;field&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;description&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;news_desk&lt;/td&gt;
      &lt;td&gt;newspaper section&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;headline&lt;/td&gt;
      &lt;td&gt;article headline&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;word_count&lt;/td&gt;
      &lt;td&gt;article word count&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;snippet&lt;/td&gt;
      &lt;td&gt;article summary&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;source&lt;/td&gt;
      &lt;td&gt;article source (i.e. The NY Times, Thomson, AP, etc)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;web_url&lt;/td&gt;
      &lt;td&gt;web address&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;search_terms&lt;/td&gt;
      &lt;td&gt;search terms used to find the article&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;pub_date&lt;/td&gt;
      &lt;td&gt;date the article was published&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The script used to collect a list of articles from the API follows. Note the search terms passed to the API:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;#!/usr/bin/env python2&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# -*- coding: utf-8 -*-&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;###################################################################&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# This script utilizes &quot;The New York Times&quot; Article API to retrieve &lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# article meta data for the given search terms and build a csv file.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# To register and get a key to use the API vist the following:&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   http://developer.nytimes.com/docs/read/article_search_api_v2&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# File:   pyArticleMetaList.py&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Author: Tom Borgstadt&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Date:   12-01-2015&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# output: selected data fields from API&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;###################################################################&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;csv&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nytimesarticle&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;articleAPI&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# the following NY Times registration key in quotes &#39;&#39;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# this one is mine but you can get yours at link above&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;api&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;articleAPI&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;b28axxxxxxxxxxxxf9c07fc6644d7xxxx:12:73xxxxx1&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;pathname&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;/home/tom/project/&#39;&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_news&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pages&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;global&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;news&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pages&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;articles&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;api&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;search&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                &lt;span class=&quot;c&quot;&gt;#fq = {&#39;source&#39;:[&#39;Reuters&#39;, &#39;AP&#39;, &#39;The New York Times&#39;, &#39;Autoweek&#39;]}&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;fl&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;_id&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;headline&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;news_desk&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;pub_date&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;snippet&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;source&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;web_url&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;word_count&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;page&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;sort&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;oldest&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;begin_date&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;end_date&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;articles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;response&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;docs&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;dic&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;dic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;search_term&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;dic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;id&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;_id&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;dic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;pub_date&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;pub_date&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;dic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;source&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;source&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;dic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;news_desk&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;news_desk&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;dic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;word_count&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;word_count&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;dic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;web_url&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;web_url&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;dic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;headline&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;headline&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;main&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;utf8&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;snippet&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;dic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;snippet&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;snippet&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;utf8&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;news&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# collect articles listing for following search terms from September 1, 2015 to present&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;search_terms&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;volkswagen scandal&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;volkswagen diesel&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;volkswagen cheating&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;vw scandal&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;vw diesel&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;vw cheating&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pathname&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;articles.meta_4.csv&#39;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;news&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;get_news&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;search_terms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;95&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20150901&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20150930&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;get_news&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;search_terms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;95&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20151001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20151031&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;get_news&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;search_terms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;95&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20151101&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20151130&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;get_news&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;search_terms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20151201&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20151231&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;news&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;wb&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dict_writer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;csv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DictWriter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dict_writer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;writeheader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dict_writer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;writerows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;news&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;retrieving-new-york-times-article-text&quot;&gt;Retrieving New York Times article text&lt;/h2&gt;

&lt;p&gt;The next step was to build the corpus of documents (articles). Articles published by The New York Times are easily retrieved with the web address of article. The following python script uses the package &lt;a href=&quot;https://github.com/codelucas/newspaper&quot; target=&quot;_blank&quot;&gt;newspaper&lt;/a&gt; to retrieve the article text.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;#!/usr/bin/env python3&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# -*- coding: utf-8 -*-&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;###################################################################&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# This script inputs a csv file of article meta data (created using&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# New York Times Article API), and uses &quot;newspaper&quot; package to&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# retrieve the article text.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# requirements: python 3.4+&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# input: list of news articles&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# output: all columns from input plus the article text in new file&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# File:   pyArticleTimes.py&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Author: Tom Borgstadt&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Date:   12-2-2015&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;###################################################################&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;csv&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sys&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;time&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;newspaper&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Article&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# input and output files setup&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pathname&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;&#39;&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# executing this script in the directory of the data files&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;inpath&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pathname&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;articles.meta.csv&#39;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;input_file&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inpath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;r&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;reader&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;csv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;outpath&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pathname&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;articles.text.ny.csv&#39;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;output_file&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outpath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;w&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;writer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;csv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;writer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# move pointer to column header row&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# write output file column headers&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;writer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;writerow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;news_desk&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;headline&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;word_count&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;snippet&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;source&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;web_url&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;search_term&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;pub_date&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;id&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;text&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;The New York Times&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;c&quot;&gt;# delay 1 secs&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;      
            &lt;span class=&quot;c&quot;&gt;# parse here&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Article&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;download&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

            &lt;span class=&quot;c&quot;&gt;# only generate output if we have article text&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;&#39;&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;c&quot;&gt;# output &lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;writer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;writerow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;input_file&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;output_file&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;retrieving-syndicated-article-text-from-thomson-reuters&quot;&gt;Retrieving syndicated article text from Thomson Reuters&lt;/h2&gt;

&lt;p&gt;Articles syndicated by The New York Times but published by Thomson Reuters in most cases were no longer active on The New York Times web site so I needed a way to look up the article on Thompson Reuters. I was able to achieve this by using &lt;a href=&quot;https://github.com/NikolaiT/GoogleScraper&quot; target=&quot;_blank&quot;&gt;GoogleScraper&lt;/a&gt;. GoogleScraper let me automate a Bing search for each Reuters article headline, find the article web link to Thomson Reuters, and retrieve the article text directly from Reuters.com. Bing was used because it currently does not throttle calls executed by a script, unlike other search engines. This worked well.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;#!/usr/bin/env python3&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# -*- coding: utf-8 -*-&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;###################################################################&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# This script inputs a csv file of article meta data (created using&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# New York Times Article API), and uses &quot;GoogleScraper&quot; package to&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# retrieve the actual Reuters link using the headline from the &lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# article meta data list. Bing is used as it does not block multiple&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# calls. Assume the top rank article is the link to use against &lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Reuters. This is accurate near 100% by including &quot;Reuters&quot; with the&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# headline as search terms.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Once the article links is found with the search it is utilized to&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# retrieve the article text with the package &quot;newspaper&quot;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# &lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# requirements: python 3.4+&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# input: list of news articles&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# output: all columns from input plus the article text in new file&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# File:   pyArticleTextReuters.py&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Author: Tom Borgstadt&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Date:   12-2-2015&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;###################################################################&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;GoogleScraper&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scrape_with_config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GoogleSearchError&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;GoogleScraper.utils&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_some_words&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;newspaper&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Article&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;csv&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;time&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sqlite3&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;lite&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sys&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&#39;use_own_ip&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&#39;keywords&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;headline goes here&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&#39;search_engines&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;bing&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,],&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&#39;num_pages_for_keyword&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&#39;scrape_method&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;http&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&#39;do_caching&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# input and output files setup&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pathname&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;&#39;&lt;/span&gt;      &lt;span class=&quot;c&quot;&gt;# executing this script in the directory of the data&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;inpath&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pathname&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;articles.meta.csv&#39;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;input_file&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inpath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;r&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;reader&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;csv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;outpath&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pathname&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;articles.text.others.csv&#39;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;output_file&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outpath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;w&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;writer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;csv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;writer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# move pointer to column header row&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# write output file column headers&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;writer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;writerow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;news_desk&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;headline&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;word_count&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;snippet&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;source&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;web_url&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;search_term&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;pub_date&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;id&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;text&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;Reuters&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;c&quot;&gt;# update the search keyword string with headline of interest&lt;/span&gt;
            &lt;span class=&quot;c&quot;&gt;# string &#39;Reuters&#39; in front of headline to ensure item first in search results&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;keywords&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39; &#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;

            &lt;span class=&quot;c&quot;&gt;# invoke google search scraper to generate list of links for the headline of interest&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;search&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scrape_with_config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

            &lt;span class=&quot;c&quot;&gt;# get link&lt;/span&gt;
            &lt;span class=&quot;c&quot;&gt;# database connection and cursor def&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;con&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;con&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lite&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;connect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;google_scraper.db&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;cur&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;con&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
            &lt;span class=&quot;c&quot;&gt;# we can always look at the last entry for the serp_id&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;cur&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;select link from link where serp_id = (select max(serp_id) from link) and rank=1;&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            
            &lt;span class=&quot;c&quot;&gt;# cursor returns tuple so convert to list&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;new_url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cur&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fetchone&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;con&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;con&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
            
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new_url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;c&quot;&gt;# parse here&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Article&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;download&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
                &lt;span class=&quot;c&quot;&gt;# only generate output if we have article text&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                    &lt;span class=&quot;c&quot;&gt;# output &lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;writer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;writerow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;input_file&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;output_file&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h1 id=&quot;topic-extraction&quot;&gt;Topic extraction&lt;/h1&gt;

&lt;p&gt;Non-Negative Matrix Factorization (NMF) and Graphlab were utilized for extracting topics. Graphlab was evaluated mainly because it offers ready made hooks for pyLDAvis which I was keen to evaluate (last section below).&lt;/p&gt;

&lt;p&gt;For the main part of my analysis using NMF topics, I went with the following settings (after all preprocessing, removing stop words, making replacements, etc):
Term Frequency Inverse Document Frequency (TFidf) vectorization with minimum document frequency of .015 and maximum document frequency of .75. The NMF topic model set for 15 topics.&lt;/p&gt;

&lt;p&gt;The following topic results from NMF:&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;master_topic&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;topic (top 5 words)&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;no. of articles&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Bosch Implication&lt;/td&gt;
      &lt;td&gt;bosch-supplier-components-engine-software&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;EPA Findings&lt;/td&gt;
      &lt;td&gt;epa-software-defeat-vehicles-agency&lt;/td&gt;
      &lt;td&gt;53&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Electric Markets&lt;/td&gt;
      &lt;td&gt;china-electric-technology-market-data&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Financial Impacts&lt;/td&gt;
      &lt;td&gt;billion-euros-costs-scandal-analysts&lt;/td&gt;
      &lt;td&gt;59&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;sales-percent-october-month-year&lt;/td&gt;
      &lt;td&gt;45&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;German Government Statements&lt;/td&gt;
      &lt;td&gt;german-transport-minister-dobrindt-berlin&lt;/td&gt;
      &lt;td&gt;73&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Global Road Tests&lt;/td&gt;
      &lt;td&gt;korea-south-ministry-test-unit&lt;/td&gt;
      &lt;td&gt;18&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;test-european-eu-commission-road&lt;/td&gt;
      &lt;td&gt;42&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Investigation and Litigation&lt;/td&gt;
      &lt;td&gt;australia-class-fitted-action-sydney&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;briefing-eastern-today-morning-pm&lt;/td&gt;
      &lt;td&gt;19&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;criminal-investigation-prosecutors-justic&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Management Statements&lt;/td&gt;
      &lt;td&gt;audi-porsche-hackenberg-thursday-cremer&lt;/td&gt;
      &lt;td&gt;30&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;winterkorn-company-executive-chief-board&lt;/td&gt;
      &lt;td&gt;68&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Owners and Dealers&lt;/td&gt;
      &lt;td&gt;vw-owners-models-mueller-dealers&lt;/td&gt;
      &lt;td&gt;50&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pollution Standards&lt;/td&gt;
      &lt;td&gt;fuel-gasoline-engines-pollution-standards&lt;/td&gt;
      &lt;td&gt;39&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Topic article counts:&lt;/p&gt;
&lt;figure&gt;&lt;a href=&quot;/images/vw-01.png&quot;&gt;&lt;img src=&quot;/images/vw-01.png&quot; /&gt;&lt;/a&gt;
&lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Topic frequency by week:&lt;/p&gt;
&lt;figure&gt;&lt;a href=&quot;/images/vw-02.png&quot;&gt;&lt;img src=&quot;/images/vw-02.png&quot; /&gt;&lt;/a&gt;
&lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;topic-visualization-with-phldavis&quot;&gt;Topic visualization with phLDAvis&lt;/h1&gt;
&lt;p&gt;Here I used a proprietary package Graphlab (trial) to model topics because it has ready interface to package pyLDAvis for the nice interactive visualization. pyLDAvis is model agnostic and in the essence of time, did not workout the munging necessary to get the NMF model topics into pyLDAvis but will do in separate later post.&lt;/p&gt;

&lt;p&gt;Topics from Graphlab displayed with pyLDAvis:&lt;/p&gt;
&lt;figure&gt;&lt;a href=&quot;/images/vw-03.png&quot;&gt;&lt;img src=&quot;/images/vw-03.png&quot; /&gt;&lt;/a&gt;
&lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;</content><author><name>Tom Borgstadt</name></author><category term="api" /><category term="googlescraper" /><category term="graphlab" /><category term="nmf" /><category term="ny times" /><category term="topic extraction" /><category term="web scraping" /><summary>We own two Volkswagen diesel cars affected by the recent scandal where Volkswagen knowingly mislead the EPA and the public about emissions produced from their “clean diesel” technology. So, I was interested to learn about public sentiment and discussion topics during the 3 months since September 18th when the scandal broke.</summary></entry><entry><title>Configure Multi-node Hadoop 2.6 Cluster</title><link href="http://blog.tomborgstadt.com//blog/2015/09/hadoop-multi" rel="alternate" type="text/html" title="Configure Multi-node Hadoop 2.6 Cluster" /><published>2015-09-15T00:00:00-05:00</published><updated>2015-09-15T00:00:00-05:00</updated><id>http://blog.tomborgstadt.com//blog/2015/09/hadoop-multi</id><content type="html" xml:base="http://blog.tomborgstadt.com//blog/2015/09/hadoop-multi">&lt;p&gt;This project is the final of a series of 3 posts beginning with the following posts:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2015/08/hadoop.html&quot;&gt;Install Hadoop 2.6 on Ubuntu 14.04&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/2015/08/hadoop-spark.html&quot;&gt;Install Spark 1.5 with Hadoop 2.6&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The concept is to take a working single node Hadoop cluster with Spark, and expand to 3 nodes, 1 master namenode and 2 slave data worker nodes.&lt;/p&gt;

&lt;p&gt;To simplify this project, each node will run as a virtual machine (VM) on a single hosted virtual environment. This makes it easier to build out the cluster. The single node cluster is modified with common configuration to all nodes and then the single node is cloned, wash and repeat to desired number of nodes. VirtualBox 4.3 is the virtualization platform running on Ubuntu 14.04.&lt;/p&gt;

&lt;p&gt;In a production environment, it is most likely that a Hadoop cluster would consist of 100s or even 1000s of nodes hosted in various environments, physical and/or virtual. I want to emphasize this project is for academic purposes only.&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#plan&quot; id=&quot;markdown-toc-plan&quot;&gt;Plan&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#create-master-and-apply-common-cluster-configuration&quot; id=&quot;markdown-toc-create-master-and-apply-common-cluster-configuration&quot;&gt;Create Master and apply common cluster configuration&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#hadoop&quot; id=&quot;markdown-toc-hadoop&quot;&gt;Hadoop&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#core-sitexml&quot; id=&quot;markdown-toc-core-sitexml&quot;&gt;core-site.xml&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#hdfs-sitexml&quot; id=&quot;markdown-toc-hdfs-sitexml&quot;&gt;hdfs-site.xml&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#yarn-sitexml&quot; id=&quot;markdown-toc-yarn-sitexml&quot;&gt;yarn-site.xml&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#mapred-sitexml&quot; id=&quot;markdown-toc-mapred-sitexml&quot;&gt;mapred-site.xml&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#set-master-slave-and-worker-nodes-config&quot; id=&quot;markdown-toc-set-master-slave-and-worker-nodes-config&quot;&gt;set master, slave, and worker nodes config&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#clear-logs&quot; id=&quot;markdown-toc-clear-logs&quot;&gt;clear logs&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#create-and-configure-slave-nodes&quot; id=&quot;markdown-toc-create-and-configure-slave-nodes&quot;&gt;Create and configure Slave nodes&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#complete-master-node-configuration&quot; id=&quot;markdown-toc-complete-master-node-configuration&quot;&gt;Complete Master node configuration&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#test-connectivity-between-nodes&quot; id=&quot;markdown-toc-test-connectivity-between-nodes&quot;&gt;Test connectivity between nodes&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#format-hdfs&quot; id=&quot;markdown-toc-format-hdfs&quot;&gt;Format HDFS&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#start-hadoop&quot; id=&quot;markdown-toc-start-hadoop&quot;&gt;Start Hadoop&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#start-spark&quot; id=&quot;markdown-toc-start-spark&quot;&gt;Start Spark&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#verify-active-hadoop-and-spark-subsystems&quot; id=&quot;markdown-toc-verify-active-hadoop-and-spark-subsystems&quot;&gt;Verify active Hadoop and Spark subsystems&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#test&quot; id=&quot;markdown-toc-test&quot;&gt;Test&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#create-start-and-shutdown-scripts&quot; id=&quot;markdown-toc-create-start-and-shutdown-scripts&quot;&gt;Create start and shutdown scripts&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;plan&quot;&gt;Plan&lt;/h2&gt;
&lt;p&gt;Each node of the hadoop cluster must communicate with other nodes. Since this cluster will run in a virtual environment on a single host the following static IP addresses will be used, yours might be different.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;VM Node(Server)&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Role&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;IP Address&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;hd-master&lt;/td&gt;
      &lt;td&gt;hdfs namenode/spark master&lt;/td&gt;
      &lt;td&gt;192.168.1.100&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;hd-slave1&lt;/td&gt;
      &lt;td&gt;hdfs datanode/spark worker&lt;/td&gt;
      &lt;td&gt;192.168.1.101&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;hd-slave2&lt;/td&gt;
      &lt;td&gt;hdfs datanode/spark worker&lt;/td&gt;
      &lt;td&gt;192.168.1.102&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;create-master-and-apply-common-cluster-configuration&quot;&gt;Create Master and apply common cluster configuration&lt;/h2&gt;
&lt;p&gt;I will begin by cloning the working single cluster VM &lt;strong&gt;hd-spark&lt;/strong&gt;, built in previous post, to new VM &lt;strong&gt;hd-master&lt;/strong&gt; using the VirtualBox administration GUI. I can save time and have a more consistent implementation if I apply common configuration to the first VM, propogate the common configuration using the cloning process, and then apply specific master and slave configuration later to individual VMs as needed.&lt;/p&gt;

&lt;p&gt;Start the new VM &lt;strong&gt;hd-master&lt;/strong&gt; and login as administrator (user &lt;strong&gt;tom&lt;/strong&gt; in my case).&lt;/p&gt;

&lt;p&gt;Note the terminal will still show the server name that &lt;strong&gt;hd-master&lt;/strong&gt; was cloned from as show below, but these next steps will correct the host name.&lt;/p&gt;

&lt;p&gt;Change hostname.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# edit file&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;tom@hd-spark:~$ &lt;/span&gt;sudo nano /etc/hostname

&lt;span class=&quot;c&quot;&gt;# change hd-spark to hd-master:&lt;/span&gt;
  hd-master

&lt;span class=&quot;c&quot;&gt;# save&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Change hosts table&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# edit file&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;tom@hd-spark:~$ &lt;/span&gt;sudo nano /etc/hosts

&lt;span class=&quot;c&quot;&gt;# remove line &#39;127.0.0.1 localhost&#39;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# set contents of hosts file with:&lt;/span&gt;
    192.168.1.100     hd-master
    192.168.1.101     hd-slave1
    192.168.1.102     hd-slave2

&lt;span class=&quot;c&quot;&gt;# save&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# exit terminal&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;tom@hd-master:~$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;exit&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Reopen terminal and login as &lt;strong&gt;hduser&lt;/strong&gt;. Note the host name is now correct.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;gp&quot;&gt;tom@hd-master:~$ &lt;/span&gt;su hduser
&lt;span class=&quot;gp&quot;&gt;hduser@hd-master:/home/tom$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd
&lt;/span&gt;hduser@hd-master:~&lt;span class=&quot;err&quot;&gt;$&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;hadoop&quot;&gt;Hadoop&lt;/h3&gt;
&lt;p&gt;Update the following Hadoop configuration files as specified.&lt;/p&gt;

&lt;h4 id=&quot;core-sitexml&quot;&gt;core-site.xml&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# edit file&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-master:~$ &lt;/span&gt;sudo nano /usr/local/hadoop/etc/hadoop/core-site.xml

&lt;span class=&quot;c&quot;&gt;# set the following properties:&lt;/span&gt;
&amp;lt;configuration&amp;gt;

  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;fs.default.name&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;hdfs://hd-master:9000/&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;

&amp;lt;/configuration&amp;gt;

&lt;span class=&quot;c&quot;&gt;# save&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4 id=&quot;hdfs-sitexml&quot;&gt;hdfs-site.xml&lt;/h4&gt;
&lt;p&gt;Update dfs.replication to 2 as data will be replicated on the 2 slave datanodes. Verify other properties as shown.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# edit file&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-master:~$ &lt;/span&gt;sudo nano /usr/local/hadoop/etc/hadoop/hdfs-site.xml

&lt;span class=&quot;c&quot;&gt;# set the following properties:&lt;/span&gt;
&amp;lt;configuration&amp;gt;

  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;dfs.replication&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;2&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;dfs.namenode.name.dir&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;file:/usr/local/hadoop_store/hdfs/namenode&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;dfs.datanode.data.dir&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;file:/usr/local/hadoop_store/hdfs/datanode&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;dfs.webhdfs.enabled&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;

&amp;lt;/configuration&amp;gt;

&lt;span class=&quot;c&quot;&gt;# save&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4 id=&quot;yarn-sitexml&quot;&gt;yarn-site.xml&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# edit file&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-master:~$ &lt;/span&gt;sudo nano /usr/local/hadoop/etc/hadoop/yarn-site.xml

&lt;span class=&quot;c&quot;&gt;# set the following properties:&lt;/span&gt;
&amp;lt;configuration&amp;gt;

  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.nodemanager.aux-services&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;mapreduce_shuffle&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;org.apache.hadoop.mapred.ShuffleHandler&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.resourcemanager.resource-tracker.address&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;hd-master:8025&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.resourcemanager.scheduler.address&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;hd-master:8035&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.resourcemanager.address&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;hd-master:8050&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;

&amp;lt;/configuration

&lt;span class=&quot;c&quot;&gt;# save&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4 id=&quot;mapred-sitexml&quot;&gt;mapred-site.xml&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# edit file&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-master:~$ &lt;/span&gt;sudo nano /usr/local/hadoop/etc/hadoop/mapred-site.xml

&lt;span class=&quot;c&quot;&gt;# set the following properties:&lt;/span&gt;
&amp;lt;configuration&amp;gt;

  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;mapred.job.tracker&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;hd-master:5431&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;mapred.framework.name&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;yarn&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;

&amp;lt;/configuration&amp;gt;

&lt;span class=&quot;c&quot;&gt;# save&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4 id=&quot;set-master-slave-and-worker-nodes-config&quot;&gt;set master, slave, and worker nodes config&lt;/h4&gt;
&lt;p&gt;The single master node, &lt;strong&gt;hd-master&lt;/strong&gt; will have both primary and secondary HDFS NameNodes. Note that SecondaryNameNode is not really a clone for the NameNode, it is meant to periodically retrieve the NameNode data from memory and persist it on the drive.&lt;/p&gt;

&lt;p&gt;List master node for Hadoop in masters config file.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# edit file&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-master:~$ &lt;/span&gt;sudo nano /usr/local/hadoop/etc/hadoop/masters

&lt;span class=&quot;c&quot;&gt;# add master node name:&lt;/span&gt;
    hd-master

&lt;span class=&quot;c&quot;&gt;# save&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;List slave nodes for Hadoop in slaves config file.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# edit file&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-master:~$ &lt;/span&gt;sudo nano /usr/local/hadoop/etc/hadoop/slaves

&lt;span class=&quot;c&quot;&gt;# add slave nodes names:&lt;/span&gt;
    hd-slave1
    hd-slave2

&lt;span class=&quot;c&quot;&gt;# save&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;List slave workers for Spark in slaves file.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# copy the template &lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-master:~$ &lt;/span&gt;sudo cp /usr/local/spark/conf/slaves.template slaves

&lt;span class=&quot;c&quot;&gt;# edit the file&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-master:~$ &lt;/span&gt;sudo nano /usr/local/spark/conf/slaves

&lt;span class=&quot;c&quot;&gt;# add slave workers names:&lt;/span&gt;
    hd-slave1
    hd-slave2

&lt;span class=&quot;c&quot;&gt;# save&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4 id=&quot;clear-logs&quot;&gt;clear logs&lt;/h4&gt;
&lt;p&gt;Since the single node cluster, &lt;strong&gt;hd-master&lt;/strong&gt;, was run previously clear the logs before cloning.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# remove all files in logs folder&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-master:~$ &lt;/span&gt;sudo rm -rf /usr/local/hadoop/logs&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;create-and-configure-slave-nodes&quot;&gt;Create and configure Slave nodes&lt;/h2&gt;
&lt;p&gt;Clone VirtualBox &lt;strong&gt;hd-master&lt;/strong&gt; to &lt;strong&gt;hd-slave1&lt;/strong&gt; using the VirtualBox manager.&lt;/p&gt;

&lt;p&gt;Again, at this point note the terminal will show the server name that &lt;strong&gt;hd-slave1&lt;/strong&gt; was cloned from as show below, but these next steps will correct the host name.&lt;/p&gt;

&lt;p&gt;Update hostname file with server name.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# edit file&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;tom@hd-master:~$ &lt;/span&gt;sudo nano /etc/hostname

&lt;span class=&quot;c&quot;&gt;# update server name:&lt;/span&gt;
    hd-slave1

&lt;span class=&quot;c&quot;&gt;# save&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# shutdown for network config&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;tom@hd-master:~$ &lt;/span&gt;sudo shutdown -P now&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;From VirtualBox manager, configure a network adapter for VM &lt;strong&gt;hd-slave1&lt;/strong&gt;.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;VBox&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Attached to:&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Purpose&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Adapter 1:&lt;/td&gt;
      &lt;td&gt;Bridged Adapter&lt;/td&gt;
      &lt;td&gt;for static IP (for HDFS)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;From Ubuntu create an IPv4 Ethernet network connection with the following configuration and assign to the adapter made available by VirtualBox.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;VBox&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Ubuntu&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Type&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;IP&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Netmask&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Gateway&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Adapter2:&lt;/td&gt;
      &lt;td&gt;eth0:&lt;/td&gt;
      &lt;td&gt;Manual&lt;/td&gt;
      &lt;td&gt;192.168.1.101&lt;/td&gt;
      &lt;td&gt;255.255.255.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Repeat steps in this section for additional slave(s), in this case &lt;strong&gt;hd-slave2&lt;/strong&gt; with IP: 192.168.1.102&lt;/p&gt;

&lt;h2 id=&quot;complete-master-node-configuration&quot;&gt;Complete Master node configuration&lt;/h2&gt;
&lt;p&gt;To have Internet connectivity for downloads, etc., it is necessary to configure two network adapters. This is done on the &lt;strong&gt;hd-master&lt;/strong&gt; node only but could also be done on slave nodes if needed.&lt;/p&gt;

&lt;p&gt;From VirtualBox manager UI, configure 2 network adapters as show below for VM &lt;strong&gt;hd-master&lt;/strong&gt;.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;VBox&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Attached to:&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Purpose&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Adapter1:&lt;/td&gt;
      &lt;td&gt;NAT&lt;/td&gt;
      &lt;td&gt;for Internet access&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Adapter2:&lt;/td&gt;
      &lt;td&gt;Bridged Adapter&lt;/td&gt;
      &lt;td&gt;for static IP&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;From Ubuntu create 2 IPv4 Ethernet network connections and assign to the adapters made available by VirtualBox.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;VBox&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Ubuntu&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Type&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;IP&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Netmask&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Gateway&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Adapter1:&lt;/td&gt;
      &lt;td&gt;eth1:&lt;/td&gt;
      &lt;td&gt;DHCP&lt;/td&gt;
      &lt;td&gt;n/a&lt;/td&gt;
      &lt;td&gt;n/a&lt;/td&gt;
      &lt;td&gt;n/a&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Adapter2:&lt;/td&gt;
      &lt;td&gt;eth0:&lt;/td&gt;
      &lt;td&gt;Manual&lt;/td&gt;
      &lt;td&gt;192.168.1.100&lt;/td&gt;
      &lt;td&gt;255.255.255.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;test-connectivity-between-nodes&quot;&gt;Test connectivity between nodes&lt;/h2&gt;
&lt;p&gt;Ensure all nodes in the cluster can communicate with other nodes.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;gp&quot;&gt;hduser@hd-master:~$ &lt;/span&gt;ping hd-slave1
&lt;span class=&quot;c&quot;&gt;# should return:&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   Reply from 192.168.1.101&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   cntrl-c to break&lt;/span&gt;

&lt;span class=&quot;gp&quot;&gt;hduser@hd-master:~$ &lt;/span&gt;ping hd-slave2
&lt;span class=&quot;c&quot;&gt;# should return:&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   Reply from 192.168.1.102&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   cntrl-c to break&lt;/span&gt;

&lt;span class=&quot;gp&quot;&gt;hduser@hd-slave1:~$ &lt;/span&gt;ping hd-master
&lt;span class=&quot;c&quot;&gt;# should return:&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   Reply from 192.168.1.100&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   cntrl-c to break&lt;/span&gt;

&lt;span class=&quot;gp&quot;&gt;hduser@hd-slave2:~$ &lt;/span&gt;ping hd-master
&lt;span class=&quot;c&quot;&gt;# should return:&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   Reply from 192.168.1.100&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   cntrl-c to break&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Ensure all nodes can ssh into other nodes without password prompt.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;gp&quot;&gt;hduser@hd-master:/home/tom$ &lt;/span&gt;ssh hd-slave1
&lt;span class=&quot;c&quot;&gt;# result:&lt;/span&gt;
hduser@hd-slave1:~&lt;span class=&quot;err&quot;&gt;$&lt;/span&gt;

&lt;span class=&quot;gp&quot;&gt;hduser@hd-slave1:/home/tom$ &lt;/span&gt;ssh hd-master
&lt;span class=&quot;c&quot;&gt;# result:&lt;/span&gt;
hduser@hd-master:~&lt;span class=&quot;err&quot;&gt;$&lt;/span&gt;

&lt;span class=&quot;gp&quot;&gt;hduser@hd-slave2:/home/tom$ &lt;/span&gt;ssh hd-master
&lt;span class=&quot;c&quot;&gt;# result:&lt;/span&gt;
hduser@hd-master:~&lt;span class=&quot;err&quot;&gt;$&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;format-hdfs&quot;&gt;Format HDFS&lt;/h2&gt;
&lt;p&gt;Format the namenode and HDFS. If prompted to re-format, answer yes.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;gp&quot;&gt;hduser@hd-master:~$ &lt;/span&gt;hadoop namenode -format&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;If a problem with the formatting procedure, compare the “clusterID= “ in each of the following files (these paths can be found by looking in hdfs-site.xml).  The clusterID must be the same in each of these files across master and slaves. If clusterID is different between these files, copy the clusterID from the namenode VERSION file to the datanode VERSION file and save. If these are different the datanodes will not start.&lt;/p&gt;

&lt;p&gt;Also ensure that the datanodeUuid is unique by slave, if not, make unique.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;nano /usr/local/hadoop_store/hdfs/namenode/current/VERSION
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;nano /usr/local/hadoop_store/hdfs/datanode/current/VERSION&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;start-hadoop&quot;&gt;Start Hadoop&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# start Hadoop daemons&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-master:~$ &lt;/span&gt;start-dfs.sh

&lt;span class=&quot;c&quot;&gt;# start MapReduce daemons: &lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-master:~$ &lt;/span&gt;start-yarn.sh

&lt;span class=&quot;c&quot;&gt;# verify Hadoop daemons on Master:&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-master:~$ &lt;/span&gt;jps
&lt;span class=&quot;c&quot;&gt;# should see: NameNode, SecondaryNameNode, Jps, ResourceManager&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# verify Hadoop daemons on Slaves:&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-slave1:~$ &lt;/span&gt;jps
&lt;span class=&quot;gp&quot;&gt;hduser@hd-slave2:~$ &lt;/span&gt;jps
&lt;span class=&quot;c&quot;&gt;# should see for each: Jps, NodeManager, DataNode&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;start-spark&quot;&gt;Start Spark&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# start Spark master&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-master-spk:~$ &lt;/span&gt;start-master.sh

&lt;span class=&quot;c&quot;&gt;# start Spark workers&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-master-spk:~$ &lt;/span&gt;start-slaves.sh&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;verify-active-hadoop-and-spark-subsystems&quot;&gt;Verify active Hadoop and Spark subsystems&lt;/h2&gt;

&lt;p&gt;Check Spark Master web UI:  http://hd-master:8080/&lt;/p&gt;

&lt;figure&gt;&lt;a href=&quot;/images/hd-master-02.png&quot;&gt;&lt;img src=&quot;/images/hd-master-02.png&quot; /&gt;&lt;/a&gt;
&lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Check Hadoop web UI:  http://hd-master:50070/&lt;/p&gt;

&lt;figure&gt;&lt;a href=&quot;/images/hd-master-03.png&quot;&gt;&lt;img src=&quot;/images/hd-master-03.png&quot; /&gt;&lt;/a&gt;
&lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure&gt;&lt;a href=&quot;/images/hd-master-04.png&quot;&gt;&lt;img src=&quot;/images/hd-master-04.png&quot; /&gt;&lt;/a&gt;
&lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Checking jps on each node shows Spark Master and Worker systems are now included.&lt;/p&gt;

&lt;figure&gt;&lt;a href=&quot;/images/hd-master-01.png&quot;&gt;&lt;img src=&quot;/images/hd-master-01.png&quot; /&gt;&lt;/a&gt;
&lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;test&quot;&gt;Test&lt;/h2&gt;

&lt;p&gt;Create text file and put on hdfs /myhdfs/mytext&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# create text file&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-master:~$  &lt;/span&gt;nano mytext

&lt;span class=&quot;c&quot;&gt;# paste in any text&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# save&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# create a new directory in HDFS&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-master:~$  &lt;/span&gt;hadoop fs -mkdir /myhdfs

&lt;span class=&quot;c&quot;&gt;# copy the local file into HDFS&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-master:~$  &lt;/span&gt;hadoop fs -put mytext /myhdfs/mytext&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Start pyspark shell&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# start pyspark shell&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-master:~$ &lt;/span&gt;pyspark&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Enter the following python code to perform a line count and word count of the text file created above. See if the results make sense based on the text in the file.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lines&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;textFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;hdfs://hd-master:9000/myhdfs/mytext&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lines_nonempty&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lines&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lines_nonempty&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;words&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lines_nonempty&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatMap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wordcounts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;words&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduceByKey&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sortByKey&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wordcounts&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;take&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;u&#39;the&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;13&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;u&#39;in&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;u&#39;and&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;u&#39;a&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;u&#39;was&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;u&#39;of&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;u&#39;to&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;u&#39;Minneapolis&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;u&#39;for&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;u&#39;It&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;quit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Verify can access text document via WebHDFS:&lt;/p&gt;

&lt;figure&gt;&lt;a href=&quot;/images/hd-master-05.png&quot;&gt;&lt;img src=&quot;/images/hd-master-05.png&quot; /&gt;&lt;/a&gt;
&lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;http://hd-master:50070/webhdfs/v1/myhdfs/mytext?op=OPEN&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Should get a prompt to “Save File”&lt;/p&gt;

&lt;h2 id=&quot;create-start-and-shutdown-scripts&quot;&gt;Create start and shutdown scripts&lt;/h2&gt;
&lt;p&gt;Need some wrappers to start and stop this whole thing.&lt;/p&gt;

&lt;p&gt;Add /home/hduser/bin to system PATH&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# edit user startup&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-master:~$ &lt;/span&gt;sudo nano ~/.bashrc 

&lt;span class=&quot;c&quot;&gt;# append the following line:&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;PATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/home/hduser/bin:&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$PATH&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# save&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# reset&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-master:~$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; ~/.bashrc&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Add a startup script.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# create bin directory&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-master:~$ &lt;/span&gt;mkdir bin

&lt;span class=&quot;c&quot;&gt;# make bin current directory&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-master:~$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;bin

&lt;span class=&quot;c&quot;&gt;# create a file in bin&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-master:~/bin$ &lt;/span&gt;sudo nano start-env.sh

&lt;span class=&quot;c&quot;&gt;# copy the following lines into the file:&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#!/bin/bash&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# start HDFS&lt;/span&gt;
    start-dfs.sh

    &lt;span class=&quot;c&quot;&gt;# start MapReduce&lt;/span&gt;
    start-yarn.sh

    &lt;span class=&quot;c&quot;&gt;# start Spark&lt;/span&gt;
    start-master.sh
    start-slaves.sh

&lt;span class=&quot;c&quot;&gt;# save&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# make the file executable&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-master:~/bin$ &lt;/span&gt;sudo chmod +x start-env.sh&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Add a shutdown script.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# create a file in bin&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-master:~/bin$ &lt;/span&gt;sudo nano stop-env.sh

&lt;span class=&quot;c&quot;&gt;# copy the following lines into the file:&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#!/bin/bash&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# stop Spark&lt;/span&gt;
    stop-slaves.sh
    stop-master.sh

    &lt;span class=&quot;c&quot;&gt;# stop MapReduce&lt;/span&gt;
    stop-yarn.sh

    &lt;span class=&quot;c&quot;&gt;# stop HDFS&lt;/span&gt;
    stop-dfs.sh

&lt;span class=&quot;c&quot;&gt;# save&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# make the file executable&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-master:~/bin$ &lt;/span&gt;sudo chmod +x stop-env.sh&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This concludes this little project!&lt;/p&gt;</content><author><name>Tom Borgstadt</name></author><category term="hadoop" /><category term="spark" /><category term="virtualbox" /><category term="ubuntu" /><summary>This project is the final of a series of 3 posts beginning with the following posts:</summary></entry><entry><title>Install Spark 1.5 with Hadoop 2.6</title><link href="http://blog.tomborgstadt.com//blog/2015/08/hadoop-spark" rel="alternate" type="text/html" title="Install Spark 1.5 with Hadoop 2.6" /><published>2015-08-23T00:00:00-05:00</published><updated>2015-08-23T00:00:00-05:00</updated><id>http://blog.tomborgstadt.com//blog/2015/08/hadoop-spark</id><content type="html" xml:base="http://blog.tomborgstadt.com//blog/2015/08/hadoop-spark">&lt;p&gt;I am installing Spark 1.5 today on a new single cluster of Hadoop 2.6, created from a previous post.&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#create-a-new-vm&quot; id=&quot;markdown-toc-create-a-new-vm&quot;&gt;Create a new VM&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#install-spark&quot; id=&quot;markdown-toc-install-spark&quot;&gt;Install Spark&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#start-spark-and-verify&quot; id=&quot;markdown-toc-start-spark-and-verify&quot;&gt;Start Spark and verify&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#start-hadoop-and-verify&quot; id=&quot;markdown-toc-start-hadoop-and-verify&quot;&gt;Start Hadoop and verify&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#test-spark-context&quot; id=&quot;markdown-toc-test-spark-context&quot;&gt;Test Spark context&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;create-a-new-vm&quot;&gt;Create a new VM&lt;/h2&gt;
&lt;p&gt;Clone previous virtual machine (VM) &lt;strong&gt;hd-single&lt;/strong&gt; to new VM &lt;strong&gt;hd-spark&lt;/strong&gt; using the VirtualBox administration GUI.&lt;/p&gt;

&lt;p&gt;Start the new VM &lt;strong&gt;hd-spark&lt;/strong&gt; and login as administrator (user &lt;strong&gt;tom&lt;/strong&gt; in my case).&lt;/p&gt;

&lt;p&gt;Change hostname.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# edit file&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;tom@hd-single:~$ &lt;/span&gt;sudo nano /etc/hostname

&lt;span class=&quot;c&quot;&gt;# change hd-single to hd-spark:&lt;/span&gt;
  hd-spark

&lt;span class=&quot;c&quot;&gt;# save&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Change hosts table.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# edit file&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;tom@hd-single:~$ &lt;/span&gt;sudo nano /etc/hosts

&lt;span class=&quot;c&quot;&gt;# clear the hosts file and make contents:&lt;/span&gt;
  127.0.0.1   hd-spark
  127.0.0.1   localhost

&lt;span class=&quot;c&quot;&gt;# save file&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# exit terminal&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;tom@hd-single:~$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;exit&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Open a terminal and change SSH authorized keys:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# login as hduser&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;tom@hd-spark:~$ &lt;/span&gt;su hduser

&lt;span class=&quot;c&quot;&gt;# change hduser@hd-single to hduser@hd-spark&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-spark:/home/tom$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd
&lt;/span&gt;&lt;span class=&quot;gp&quot;&gt;hduser@hd-spark:~$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; .ssh
&lt;span class=&quot;gp&quot;&gt;hduser@hd-spark:~/.ssh$ &lt;/span&gt;nano authorized_keys

&lt;span class=&quot;c&quot;&gt;# change &#39;hduser@hd-single&#39; to &#39;hduser@hd-spark&#39;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# save&lt;/span&gt;

&lt;span class=&quot;gp&quot;&gt;hduser@hd-spark:~/.ssh$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;install-spark&quot;&gt;Install Spark&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# download Spark&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-spark:~$ &lt;/span&gt;wget http://d3kbcqa49mib13.cloudfront.net/spark-1.5.0-bin-hadoop2.6.tgz

&lt;span class=&quot;c&quot;&gt;# untar (unzip)&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-spark:~$ &lt;/span&gt;tar -xvf spark-1.5.0-bin-hadoop2.6.tgz

&lt;span class=&quot;c&quot;&gt;# create Spark directory&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-spark:~$ &lt;/span&gt;sudo mkdir /usr/local/spark

&lt;span class=&quot;c&quot;&gt;# change working directory&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-spark:~$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;spark-1.5.0-bin-hadoop2.6

&lt;span class=&quot;c&quot;&gt;# move the contents of Spark installation to the /usr/local/spark directory:&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-spark:~/spark-1.5.0-bin-hadoop2.6$ &lt;/span&gt;sudo mv &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; /usr/local/spark

&lt;span class=&quot;c&quot;&gt;# change owner of the spark directory&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-spark:~/spark-1.5.0-bin-hadoop2.6$ &lt;/span&gt;sudo chown -R hduser:hadoop /usr/local/spark

&lt;span class=&quot;c&quot;&gt;# edit ~/.bashrc so Spark environment&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-spark:~$ &lt;/span&gt;sudo nano ~/.bashrc

&lt;span class=&quot;c&quot;&gt;# add the following lines:&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;# SPARK VARIABLES BEGIN&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;SPARK_HOME&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/usr/local/spark
    &lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;PATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$PATH&lt;/span&gt;:&lt;span class=&quot;nv&quot;&gt;$SPARK_HOME&lt;/span&gt;/bin
    &lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;PATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$PATH&lt;/span&gt;:&lt;span class=&quot;nv&quot;&gt;$SPARK_HOME&lt;/span&gt;/sbin
  &lt;span class=&quot;c&quot;&gt;# SPARK VARIABLES END&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# save ~/.bashrc&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# refresh environment&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hdp-single-spk:~$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; ~/.bashrc

&lt;span class=&quot;c&quot;&gt;# install is complete&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;start-spark-and-verify&quot;&gt;Start Spark and verify&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# start Spark master&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-spark:~$ &lt;/span&gt;start-master.sh

&lt;span class=&quot;c&quot;&gt;# start Spark workers&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-spark:~$ &lt;/span&gt;start-slaves.sh&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Open a browser and navigate to Spark Administration.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Spark Admin&lt;/td&gt;
      &lt;td&gt;http://hd-spark:8080/&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;URL:&lt;/td&gt;
      &lt;td&gt;spark://hd-spark:7077&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;REST URL:&lt;/td&gt;
      &lt;td&gt;spark://hd-spark:6066 (cluster mode)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;start-hadoop-and-verify&quot;&gt;Start Hadoop and verify&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# start Hadoop daemons&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-master:~$ &lt;/span&gt;start-dfs.sh

&lt;span class=&quot;c&quot;&gt;# start MapReduce daemons: &lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-master:~$ &lt;/span&gt;start-yarn.sh&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Check the cluster status. Open a browser and navigate to the following URLs:&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;http://hd-spark:50070/&lt;/td&gt;
      &lt;td&gt;web UI of the NameNode daemon&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Spark Administration.&lt;/p&gt;
&lt;figure&gt;&lt;a href=&quot;/images/hd-spark-01.png&quot;&gt;&lt;img src=&quot;/images/hd-spark-01.png&quot; /&gt;&lt;/a&gt;
&lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;NameNode Status.&lt;/p&gt;
&lt;figure&gt;&lt;a href=&quot;/images/hd-spark-02.png&quot;&gt;&lt;img src=&quot;/images/hd-spark-02.png&quot; /&gt;&lt;/a&gt;
&lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;DataNode Status.&lt;/p&gt;
&lt;figure&gt;&lt;a href=&quot;/images/hd-spark-03.png&quot;&gt;&lt;img src=&quot;/images/hd-spark-03.png&quot; /&gt;&lt;/a&gt;
&lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;test-spark-context&quot;&gt;Test Spark context&lt;/h2&gt;

&lt;p&gt;Create text file and put on hdfs /myhdfs/mytext&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# create text file&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-spark:~$  &lt;/span&gt;nano mytext

&lt;span class=&quot;c&quot;&gt;# paste in any text&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# save&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# create a new directory in HDFS&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-spark:~$  &lt;/span&gt;hadoop fs -mkdir /myhdfs

&lt;span class=&quot;c&quot;&gt;# copy the local file into HDFS&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-spark:~$  &lt;/span&gt;hadoop fs -put mytext /myhdfs/mytext&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Start pyspark shell.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# start pyspark shell&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-spark:~$ &lt;/span&gt;pyspark&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Enter the following python code to perform a line count and word count of the text file created above. See if the results make sense based on the text in the file.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lines&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;textFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;hdfs://hd-spark:9000/myhdfs/mytext&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lines_nonempty&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lines&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lines_nonempty&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;words&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lines_nonempty&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatMap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wordcounts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;words&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduceByKey&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sortByKey&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wordcounts&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;take&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;u&#39;the&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;13&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;u&#39;in&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;u&#39;and&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;u&#39;a&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;u&#39;was&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;u&#39;of&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;u&#39;to&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;u&#39;Minneapolis&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;u&#39;for&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;u&#39;It&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;quit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;That is it! Spark is ready to use.&lt;/p&gt;</content><author><name>Tom Borgstadt</name></author><category term="hadoop" /><category term="spark" /><category term="ubuntu" /><category term="virtualbox" /><summary>I am installing Spark 1.5 today on a new single cluster of Hadoop 2.6, created from a previous post.</summary></entry><entry><title>Install Hadoop 2.6 on Ubuntu 14.04</title><link href="http://blog.tomborgstadt.com//blog/2015/08/hadoop" rel="alternate" type="text/html" title="Install Hadoop 2.6 on Ubuntu 14.04" /><published>2015-08-15T00:00:00-05:00</published><updated>2015-08-15T00:00:00-05:00</updated><id>http://blog.tomborgstadt.com//blog/2015/08/hadoop</id><content type="html" xml:base="http://blog.tomborgstadt.com//blog/2015/08/hadoop">&lt;p&gt;The purpose of this project was twofold. First, to learn how to install Hadoop, and second, to prepare for a second project to expand to a multi-node implementation.&lt;/p&gt;

&lt;p&gt;This is all linux command line. Use the text editor of your choice. I use nano.&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#prerequisites&quot; id=&quot;markdown-toc-prerequisites&quot;&gt;Prerequisites&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#base-environment&quot; id=&quot;markdown-toc-base-environment&quot;&gt;Base Environment&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#install-java&quot; id=&quot;markdown-toc-install-java&quot;&gt;Install JAVA&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#disable-ipv6&quot; id=&quot;markdown-toc-disable-ipv6&quot;&gt;Disable IPv6&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#install-secure-shell-ssh&quot; id=&quot;markdown-toc-install-secure-shell-ssh&quot;&gt;Install Secure Shell (SSH)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#create-hadoop-user&quot; id=&quot;markdown-toc-create-hadoop-user&quot;&gt;Create Hadoop User&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#create-ssh-certificates&quot; id=&quot;markdown-toc-create-ssh-certificates&quot;&gt;Create SSH Certificates&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#install-hadoop&quot; id=&quot;markdown-toc-install-hadoop&quot;&gt;Install Hadoop&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#bashrc&quot; id=&quot;markdown-toc-bashrc&quot;&gt;~/.bashrc&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#hadoop-envsh&quot; id=&quot;markdown-toc-hadoop-envsh&quot;&gt;hadoop-env.sh&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#core-sitexml&quot; id=&quot;markdown-toc-core-sitexml&quot;&gt;core-site.xml&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#mapred-sitexml&quot; id=&quot;markdown-toc-mapred-sitexml&quot;&gt;mapred-site.xml&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#hdfs-sitexml&quot; id=&quot;markdown-toc-hdfs-sitexml&quot;&gt;hdfs-site.xml&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#yarn-sitexml&quot; id=&quot;markdown-toc-yarn-sitexml&quot;&gt;yarn-site.xml&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#format-hadoop-filesystem&quot; id=&quot;markdown-toc-format-hadoop-filesystem&quot;&gt;Format Hadoop Filesystem&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#start-hadoop-and-verify&quot; id=&quot;markdown-toc-start-hadoop-and-verify&quot;&gt;Start Hadoop and Verify&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#hadoop-browser-interfaces&quot; id=&quot;markdown-toc-hadoop-browser-interfaces&quot;&gt;Hadoop Browser Interfaces&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;prerequisites&quot;&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;There are some basic requirements before the installation and configuration of Hadoop can begin.&lt;/p&gt;

&lt;h3 id=&quot;base-environment&quot;&gt;Base Environment&lt;/h3&gt;
&lt;p&gt;Hadoop is installed here on a virtual machine (VM) running Ubuntu 14.04 on VirtualBox 4.3.30. If you are installing your single node instance on physical hardware then there is no need for VirtualBox. However, virtualization is a powerful tool for these sorts of projects especially in the exploratory or proof of concept stages.&lt;/p&gt;

&lt;p&gt;The server name I used is &lt;strong&gt;hd-single&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;I have included the command line context throughout this writing. For example &lt;strong&gt;hduser@hd-single&lt;/strong&gt; indicates that user &lt;strong&gt;hduser&lt;/strong&gt; is the active user on server &lt;strong&gt;hd-single&lt;/strong&gt;. Also note, user &lt;strong&gt;tom&lt;/strong&gt; is the server administrator used to install prerequisite software.&lt;/p&gt;

&lt;h3 id=&quot;install-java&quot;&gt;Install JAVA&lt;/h3&gt;
&lt;p&gt;Hadoop requires Java. The open JDK project is the default version of Java that is provided from the supported Ubuntu repository. This should work.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# install jdk&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;tom@hd-single:~$ &lt;/span&gt;sudo apt-get install default-jdk

&lt;span class=&quot;c&quot;&gt;# check version&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;tom@hd-single:~$ &lt;/span&gt;java -version
&lt;span class=&quot;c&quot;&gt;# returns something like:&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   java version &quot;1.7.0_79&quot;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   OpenJDK Runtime Environment (IcedTea 2.5.6) (7u79-2.5.6-0ubuntu1.14.04.1)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   OpenJDK 64-Bit Server VM (build 24.79-b02, mixed mode)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;disable-ipv6&quot;&gt;Disable IPv6&lt;/h3&gt;
&lt;p&gt;Hadoop may bind to IPv6 addresses. Hadoop will not work on IPv6. Disable IPv6.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# edit systctl.conf&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;tom@hd-single:~/$ &lt;/span&gt;sudo nano /etc/sysctl.conf

&lt;span class=&quot;c&quot;&gt;# add the following lines at the end of the file:&lt;/span&gt;
	net.ipv6.conf.all.disable_ipv6 &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 1
	net.ipv6.conf.default.disable_ipv6 &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 1
	net.ipv6.conf.lo.disable_ipv6 &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 1

&lt;span class=&quot;c&quot;&gt;# save sysctl.conf&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# reboot&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;tom@hd-single:~/$ &lt;/span&gt;sudo shutdown -r now

&lt;span class=&quot;c&quot;&gt;# after reboot, check return value should be 1&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;tom@hd-single$ &lt;/span&gt;cat /proc/sys/net/ipv6/conf/all/disable_ipv6
&lt;span class=&quot;c&quot;&gt;# returns:&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   1&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;install-secure-shell-ssh&quot;&gt;Install Secure Shell (SSH)&lt;/h3&gt;
&lt;p&gt;Hadoop requires SSH for access to its nodes. SSH has two main components.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The command used to connect to remote machines - the client&lt;/li&gt;
  &lt;li&gt;The daemon that runs on the node and allows the client to connect&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# install ssh&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;tom@hd-single:~$ &lt;/span&gt;sudo apt-get install ssh

&lt;span class=&quot;c&quot;&gt;# check install by the following commands and note results&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;tom@hd-single:~$ &lt;/span&gt;which ssh
&lt;span class=&quot;c&quot;&gt;# returns:&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   /usr/bin/ssh&lt;/span&gt;

&lt;span class=&quot;gp&quot;&gt;tom@hd-single:~$ &lt;/span&gt;which sshd
&lt;span class=&quot;c&quot;&gt;# returns: &lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   /usr/sbin/sshd&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# these look ok&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;create-hadoop-user&quot;&gt;Create Hadoop User&lt;/h3&gt;
&lt;p&gt;Create a dedicated Hadoop group &lt;strong&gt;hadoop&lt;/strong&gt; and user &lt;strong&gt;hduser&lt;/strong&gt;. Hadoop will run under this user and this user will be used by Hadoop to access future nodes with SSH.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# create group&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;tom@hd-single:~$ &lt;/span&gt;sudo addgroup hadoop

&lt;span class=&quot;c&quot;&gt;# create user and add user to group&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;tom@hd-single:~$ &lt;/span&gt;sudo adduser --ingroup hadoop hduser

&lt;span class=&quot;c&quot;&gt;# add the user to the sudoers (administrators) group&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;tom@hd-single:~$ &lt;/span&gt;sudo adduser hduser sudo&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;create-ssh-certificates&quot;&gt;Create SSH Certificates&lt;/h3&gt;
&lt;p&gt;Since this is a single node setup, SSH only needs configuration for local host. SSH needs to be up and running and configured to allow SSH public key authentication. In other words, a certificate needs to be created so &lt;strong&gt;hduser&lt;/strong&gt; is not prompted for a password.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# login as hduser&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;tom@hd-single:~$ &lt;/span&gt;su hduser

&lt;span class=&quot;c&quot;&gt;# back to root&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-single:/home/tom$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; 

&lt;span class=&quot;c&quot;&gt;# generate public key, leaving all prompts &quot;blank&quot; for this purpose&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# this will create public key /home/hduser/.ssh/id_rsa.pub&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-single:~$ &lt;/span&gt;ssh-keygen -t rsa -P &lt;span class=&quot;s2&quot;&gt;&quot;&quot;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# add the new key to the list of authorized keys&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-single:~$ &lt;/span&gt;cat &lt;span class=&quot;nv&quot;&gt;$HOME&lt;/span&gt;/.ssh/id_rsa.pub &amp;gt;&amp;gt; &lt;span class=&quot;nv&quot;&gt;$HOME&lt;/span&gt;/.ssh/authorized_keys

&lt;span class=&quot;c&quot;&gt;# check if ssh works, enter &quot;yes&quot; when prompted &quot;Are you sure...?&quot;&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-single:~$ &lt;/span&gt;ssh localhost
&lt;span class=&quot;c&quot;&gt;# returns something like:&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   The authenticity of host &#39;localhost (127.0.0.1)&#39; can&#39;t be established.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   ECDSA key fingerprint is e1:8b:a0:a5:75:ef:f4:b4:5e:a9:ed:be:64:be:5c:2f.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   Are you sure you want to continue connecting (yes/no)? yes&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   Warning: Permanently added &#39;localhost&#39; (ECDSA) to the list of known hosts.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   Welcome to Ubuntu 14.04.1 LTS (GNU/Linux 3.13.0-40-generic x86_64)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;install-hadoop&quot;&gt;Install Hadoop&lt;/h2&gt;
&lt;p&gt;The system is now ready to install Hadoop. I installed Hadoop to &lt;strong&gt;/usr/local/hadoop&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# download Hadoop&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-single:~$ &lt;/span&gt;wget http://mirrors.sonic.net/apache/hadoop/common/hadoop-2.6.0/hadoop-2.6.0.tar.gz

&lt;span class=&quot;c&quot;&gt;# untar (unzip)&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-single:~$ &lt;/span&gt;tar xvzf hadoop-2.6.0.tar.gz

&lt;span class=&quot;c&quot;&gt;# make hadoop directory&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-single:~$ &lt;/span&gt;sudo mkdir /usr/local/hadoop

&lt;span class=&quot;c&quot;&gt;# change directory&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-single:~$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;hadoop-2.6.0

&lt;span class=&quot;c&quot;&gt;# move the contents of Hadoop installation to the /usr/local/hadoop directory:&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-single:~/hadoop-2.6.0$ &lt;/span&gt;sudo mv &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; /usr/local/hadoop

&lt;span class=&quot;c&quot;&gt;# change owner of the hadoop directory&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-single:~/hadoop-2.6.0$ &lt;/span&gt;sudo chown -R hduser:hadoop /usr/local/hadoop

&lt;span class=&quot;c&quot;&gt;# back to root&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-single:~/hadoop-2.6.0$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; ..&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;##Configure Hadoop
The following files must be modified to complete the Hadoop setup.&lt;/p&gt;

&lt;h4 id=&quot;bashrc&quot;&gt;~/.bashrc&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# find the path where Java resides to set the JAVA_HOME environment variable&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-single:~$ &lt;/span&gt;update-alternatives --config java
&lt;span class=&quot;c&quot;&gt;# returns something like:&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   There is only one alternative in link group java (providing /usr/bin/java): &lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   /usr/lib/jvm/java-7-openjdk-amd64/jre/bin/java&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   Nothing to configure.&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# make note path for JAVA_HOME&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# edit file&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-single:~$ &lt;/span&gt;nano ~/.bashrc

&lt;span class=&quot;c&quot;&gt;# append the following lines to ~/.bashrc&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# this will set the environment when the user logs into Ubuntu&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   ensure JAVA_HOME is set correctly with info from alternatives info above&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   ensure HADOOP_HOME is set correctly with path from install&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;# HADOOP VARIABLES START&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;JAVA_HOME&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/usr/lib/jvm/java-7-openjdk-amd64
    &lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_HOME&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/usr/local/hadoop

    &lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;PATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$PATH&lt;/span&gt;:&lt;span class=&quot;nv&quot;&gt;$HADOOP_HOME&lt;/span&gt;/bin
    &lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;PATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$PATH&lt;/span&gt;:&lt;span class=&quot;nv&quot;&gt;$HADOOP_HOME&lt;/span&gt;/sbin
    &lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_MAPRED_HOME&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$HADOOP_HOME&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_COMMON_HOME&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$HADOOP_HOME&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_HDFS_HOME&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$HADOOP_HOME&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;YARN_HOME&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$HADOOP_HOME&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_COMMON_LIB_NATIVE_DIR&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$HADOOP_HOME&lt;/span&gt;/lib/native
    &lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_OPTS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;-Djava.library.path=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$HADOOP_HOME&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/lib&quot;&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;# HADOOP VARIABLES END&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# save ~/.bashrc&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# if other users will be using hadoop this will need to be repeated for each&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# re-set active environment after adding above&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-single:~$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; ~/.bashrc

&lt;span class=&quot;c&quot;&gt;# check environment variables:&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-single:~$ &lt;/span&gt;printenv&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4 id=&quot;hadoop-envsh&quot;&gt;hadoop-env.sh&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# edit file&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-single:~$ &lt;/span&gt;nano /usr/local/hadoop/etc/hadoop/hadoop-env.sh

&lt;span class=&quot;c&quot;&gt;# modify following line to reflect correct path for JAVA:&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;JAVA_HOME&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/usr/lib/jvm/java-7-openjdk-amd64

&lt;span class=&quot;c&quot;&gt;# save&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# this will ensure JAVA is available to Hadoop at startup&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4 id=&quot;core-sitexml&quot;&gt;core-site.xml&lt;/h4&gt;
&lt;p&gt;This file contains configuration properties for Hadoop at startup. This file overrides default settings.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# create a directory for hadoop to override settings&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-single:~$ &lt;/span&gt;sudo mkdir -p /app/hadoop/tmp

&lt;span class=&quot;c&quot;&gt;# change owner&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-single:~$ &lt;/span&gt;sudo chown hduser:hadoop /app/hadoop/tmp

&lt;span class=&quot;c&quot;&gt;# edit file&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-single:~$ &lt;/span&gt;nano /usr/local/hadoop/etc/hadoop/core-site.xml

&lt;span class=&quot;c&quot;&gt;# set the following property:&lt;/span&gt;
&amp;lt;configuration&amp;gt;

  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;fs.default.name&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;hdfs://localhost:9000&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;

&amp;lt;/configuration&amp;gt;

&lt;span class=&quot;c&quot;&gt;# save&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4 id=&quot;mapred-sitexml&quot;&gt;mapred-site.xml&lt;/h4&gt;
&lt;p&gt;By default, the /usr/local/hadoop/etc/hadoop/ folder contains /usr/local/hadoop/etc/hadoop/mapred-site.xml.template file which must be renamed/copied with the name mapred-site.xml. The mapred-site.xml file is used to specify which framework is being used for MapReduce. We will use the YARN framework.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# copy&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-single:~$ &lt;/span&gt;cp /usr/local/hadoop/etc/hadoop/mapred-site.xml.template /usr/local/hadoop/etc/hadoop/mapred-site.xml

&lt;span class=&quot;c&quot;&gt;# edit file&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-single:~$ &lt;/span&gt;nano /usr/local/hadoop/etc/hadoop/mapred-site.xml

&lt;span class=&quot;c&quot;&gt;# set the following property:&lt;/span&gt;
&amp;lt;configuration&amp;gt;

  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;mapreduce.framework.name&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;yarn&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;

&amp;lt;/configuration&amp;gt;

&lt;span class=&quot;c&quot;&gt;# save&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4 id=&quot;hdfs-sitexml&quot;&gt;hdfs-site.xml&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# create two directories which will contain&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# the namenode and the datanode for this installation&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-single:~$ &lt;/span&gt;sudo mkdir -p /usr/local/hadoop_store/hdfs/namenode
&lt;span class=&quot;gp&quot;&gt;hduser@hd-single:~$ &lt;/span&gt;sudo mkdir -p /usr/local/hadoop_store/hdfs/datanode

&lt;span class=&quot;c&quot;&gt;# change owner of the store&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-single:~$ &lt;/span&gt;sudo chown -R hduser:hadoop /usr/local/hadoop_store

&lt;span class=&quot;c&quot;&gt;# edit file&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-single:~$ &lt;/span&gt;nano /usr/local/hadoop/etc/hadoop/hdfs-site.xml

&lt;span class=&quot;c&quot;&gt;# set the following properties:&lt;/span&gt;
&amp;lt;configuration&amp;gt;

  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;dfs.replication&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;1&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;dfs.namenode.name.dir&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;file:/usr/local/hadoop_store/hdfs/namenode&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;dfs.datanode.data.dir&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;file:/usr/local/hadoop_store/hdfs/datanode&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;dfs.webhdfs.enabled&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;

&amp;lt;/configuration&amp;gt;

&lt;span class=&quot;c&quot;&gt;# save&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4 id=&quot;yarn-sitexml&quot;&gt;yarn-site.xml&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# edit file&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-single:~$ &lt;/span&gt;nano /usr/local/hadoop/etc/hadoop/yarn-site.xml

&lt;span class=&quot;c&quot;&gt;# set the following property:&lt;/span&gt;
&amp;lt;configuration&amp;gt;

  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.nodemanager.aux-services&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;mapreduce_shuffle&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;org.apache.hadoop.mapred.ShuffleHandler&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;

&amp;lt;/configuration&amp;gt;

&lt;span class=&quot;c&quot;&gt;# save&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;format-hadoop-filesystem&quot;&gt;Format Hadoop Filesystem&lt;/h2&gt;
&lt;p&gt;The format command should be issued with write permission since it creates directory under /usr/local/hadoop_store/hdfs/namenode folder.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# format HDFS&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-single:~$ &lt;/span&gt;hadoop namenode -format&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;start-hadoop-and-verify&quot;&gt;Start Hadoop and Verify&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# start hdfs:&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-single:~$ &lt;/span&gt;start-dfs.sh

&lt;span class=&quot;c&quot;&gt;# may see a warning &lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# (WARN util.NativeCodeLoader: Unable....using builtin-java classes where applicable)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# this is ok&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# start MapReduce (running on the Yarn)&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-single:~$ &lt;/span&gt;start-yarn.sh

&lt;span class=&quot;c&quot;&gt;# verify all daemons&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-single:/usr/local/hadoop/sbin$ &lt;/span&gt;jps
&lt;span class=&quot;c&quot;&gt;# should see:&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   Jps&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   NameNode&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   SecondaryNameNode&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   ResourceManager&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   DataNode&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   NodeManager&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# check Hadoop version&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;hduser@hd-single:~$ &lt;/span&gt;hadoop version
&lt;span class=&quot;c&quot;&gt;# returns:&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   Hadoop 2.6.0&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   Subversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r &lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   Compiled by jenkins on 2014-11-13T21:10Z&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   Compiled with protoc 2.5.0&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   From source with checksum 18e43357c8f927c0695f1e9522859d6a&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   This command was run using /usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0.jar&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;hadoop-browser-interfaces&quot;&gt;Hadoop Browser Interfaces&lt;/h2&gt;
&lt;p&gt;YARN must be active to access the following default URLs.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Administration&lt;/td&gt;
      &lt;td&gt;http://hd-single:8088/&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;NameNode Status&lt;/td&gt;
      &lt;td&gt;http://hd-single:50070/dfshealth.html#tab-overview&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;DataNode Status&lt;/td&gt;
      &lt;td&gt;http://hd-single:50070/dfshealth.html#tab-datanode&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Administration&lt;/p&gt;
&lt;figure&gt;&lt;a href=&quot;/images/hd-single-01.png&quot;&gt;&lt;img src=&quot;/images/hd-single-01.png&quot; /&gt;&lt;/a&gt;
&lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;NameNode Status&lt;/p&gt;
&lt;figure&gt;&lt;a href=&quot;/images/hd-single-02.png&quot;&gt;&lt;img src=&quot;/images/hd-single-02.png&quot; /&gt;&lt;/a&gt;
&lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;DataNode Status&lt;/p&gt;
&lt;figure&gt;&lt;a href=&quot;/images/hd-single-03.png&quot;&gt;&lt;img src=&quot;/images/hd-single-03.png&quot; /&gt;&lt;/a&gt;
&lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;This completes the installation.&lt;/p&gt;</content><author><name>Tom Borgstadt</name></author><category term="hadoop" /><category term="ubuntu" /><category term="virtualbox" /><summary>The purpose of this project was twofold. First, to learn how to install Hadoop, and second, to prepare for a second project to expand to a multi-node implementation.</summary></entry><entry><title>Predicting Diabetes Hospital Patient Re-admittance</title><link href="http://blog.tomborgstadt.com//blog/2015/05/diabetes" rel="alternate" type="text/html" title="Predicting Diabetes Hospital Patient Re-admittance" /><published>2015-05-15T00:00:00-05:00</published><updated>2015-05-15T00:00:00-05:00</updated><id>http://blog.tomborgstadt.com//blog/2015/05/diabetes</id><content type="html" xml:base="http://blog.tomborgstadt.com//blog/2015/05/diabetes">&lt;p&gt;The objective here is to see if its possible to identify diabetes patients that would be likely to readmit to the hospital. If so, there is tremendous opportunity to reduce costs associated with hospital stays even if re-admittances could be reduced slightly.&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#dataset&quot; id=&quot;markdown-toc-dataset&quot;&gt;Dataset&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#modeling-approach&quot; id=&quot;markdown-toc-modeling-approach&quot;&gt;Modeling Approach&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#analysis&quot; id=&quot;markdown-toc-analysis&quot;&gt;Analysis&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#results&quot; id=&quot;markdown-toc-results&quot;&gt;Results&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conclusion&quot; id=&quot;markdown-toc-conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dataset&quot;&gt;Dataset&lt;/h2&gt;
&lt;p&gt;The dataset I utilized was from the UCI Machine Learning Repository:
&lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008&quot; target=&quot;_blank&quot;&gt;Beata Strack, Jonathan P. DeShazo, Chris Gennings, Juan L. Olmo, Sebastian Ventura, Krzysztof J. Cios, and John N. Clore, “Impact of HbA1c Measurement on Hospital Readmission Rates: Analysis of 70,000 Clinical Database Patient Records,” BioMed Research International, vol. 2014, Article ID 781670, 11 pages, 2014.&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;10 years of clinical diabetes inpatient care at 130 US hospitals&lt;/li&gt;
  &lt;li&gt;101,766 instances, 55 attributes&lt;/li&gt;
  &lt;li&gt;Each instance recorded whenever diabetes was entered as a diagnosis&lt;/li&gt;
  &lt;li&gt;Laboratory test results&lt;/li&gt;
  &lt;li&gt;Medications administered&lt;/li&gt;
  &lt;li&gt;Length of stay, type of admittance, visits prior to admittance&lt;/li&gt;
  &lt;li&gt;Basic demographics&lt;/li&gt;
  &lt;li&gt;Readmitted =&amp;gt; No, Yes(&amp;gt;30 days), Yes(&amp;lt;30 days)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;modeling-approach&quot;&gt;Modeling Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Use an out of bag estimate for initial feasibility assessment&lt;/li&gt;
  &lt;li&gt;Fit a random forest model based on initial variable selection and tune using cross validation on a 60% split training set&lt;/li&gt;
  &lt;li&gt;For comparison fit a decision tree model using the same variable selection and training set&lt;/li&gt;
  &lt;li&gt;Tune =&amp;gt; Analyze Results =&amp;gt; Tune Again =&amp;gt;…. Etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The R notebook can be view &lt;a href=&quot;/notebooks/20150515-rDiabetes.html&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;analysis&quot;&gt;Analysis&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Difficulty improving accuracy greater than 62%&lt;/li&gt;
  &lt;li&gt;Data has many categorical variables with large number of levels which model has bias but variables don’t provide best splitting values&lt;/li&gt;
  &lt;li&gt;Experiment:  Use clustering to identify groups that have clear tendency to re-admit and use the group identifier (cluster) to achieve better ensemble predictive performance&lt;/li&gt;
  &lt;li&gt;Clustering identifies clear relationship (quantitative) between the following variables:
    &lt;ul&gt;
      &lt;li&gt;Gender: Diabetes Meds: Age: Max Glucose&lt;/li&gt;
      &lt;li&gt;A1 Cresult: Insulin: No of Diagnosis: Readmitted&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;figure&gt;&lt;a href=&quot;/images/diabetes-01.png&quot;&gt;&lt;img src=&quot;/images/20150515-diabetes-01.png&quot; /&gt;&lt;/a&gt;
&lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;DT Boosted&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;predicted patient would re-admit and they did re-admit -&amp;gt; 100% of the time&lt;/li&gt;
  &lt;li&gt;predicted patient would not re-admit and they did NOT re-admit -&amp;gt; 79.9% of the time&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Models optimized with clustering (K-means) to identify groups at risk for re-admittance&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Predicting patients at risk for re-admittance seems achievable however the data set has been challenging&lt;/li&gt;
  &lt;li&gt;Even less than desirable accuracy could make significant impact from a business/cost perspective&lt;/li&gt;
  &lt;li&gt;The method used to boost performance by clustering presents an interesting idea but needs further research to validate&lt;/li&gt;
  &lt;li&gt;Research additional methods for improving random forest performance:
    &lt;ul&gt;
      &lt;li&gt;Partial permutations for variable selection&lt;/li&gt;
      &lt;li&gt;Using combination of decision tree learning and clustering&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Tom Borgstadt</name></author><category term="k-means" /><category term="rpart" /><category term="classification" /><summary>The objective here is to see if its possible to identify diabetes patients that would be likely to readmit to the hospital. If so, there is tremendous opportunity to reduce costs associated with hospital stays even if re-admittances could be reduced slightly.</summary></entry></feed>
