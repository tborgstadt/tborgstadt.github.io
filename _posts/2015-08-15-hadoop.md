---
layout: post
title: Install Hadoop 2.6 on Ubuntu 14.04
description: This is an installation script for single node of Hadoop 2.6
tags: [hadoop, ubuntu, virtualbox]
comments: False
---
The purpose of this project was twofold. First, to learn how to install Hadoop, and second, to prepare for a second project to expand to a multi-node implementation.

This is all linux command line. Use the text editor of your choice. I use nano.

* TOC
{:toc}

## Prerequisites
There are some basic requirements before the installation and configuration of Hadoop can begin.

### Base Environment
Hadoop is installed here on a virtual machine (VM) running Ubuntu 14.04 on VirtualBox 4.3.30. If you are installing your single node instance on physical hardware then there is no need for VirtualBox. However, virtualization is a powerful tool for these sorts of projects especially in the exploratory or proof of concept stages.

The server name I used is **hd-single**.

I have included the command line context throughout this writing. For example **hduser@hd-single** indicates that user **hduser** is the active user on server **hd-single**. Also note, user **tom** is the server administrator used to install prerequisite software.

### Install JAVA
Hadoop requires Java. The open JDK project is the default version of Java that is provided from the supported Ubuntu repository. This should work.
{% highlight bash %}
# install jdk
tom@hd-single:~$ sudo apt-get install default-jdk

# check version
tom@hd-single:~$ java -version
# returns something like:
#   java version "1.7.0_79"
#   OpenJDK Runtime Environment (IcedTea 2.5.6) (7u79-2.5.6-0ubuntu1.14.04.1)
#   OpenJDK 64-Bit Server VM (build 24.79-b02, mixed mode)
{% endhighlight %}

### Disable IPv6
Hadoop may bind to IPv6 addresses. Hadoop will not work on IPv6. Disable IPv6.
{% highlight bash %}
# edit systctl.conf
tom@hd-single:~/$ sudo nano /etc/sysctl.conf

# add the following lines at the end of the file:
	net.ipv6.conf.all.disable_ipv6 = 1
	net.ipv6.conf.default.disable_ipv6 = 1
	net.ipv6.conf.lo.disable_ipv6 = 1

# save sysctl.conf

# reboot
tom@hd-single:~/$ sudo shutdown -r now

# after reboot, check return value should be 1
tom@hd-single$ cat /proc/sys/net/ipv6/conf/all/disable_ipv6
# returns:
#   1
{% endhighlight %}

### Install Secure Shell (SSH)
Hadoop requires SSH for access to its nodes. SSH has two main components.

* The command used to connect to remote machines - the client
* The daemon that runs on the node and allows the client to connect

{% highlight bash %}
# install ssh
tom@hd-single:~$ sudo apt-get install ssh

# check install by the following commands and note results
tom@hd-single:~$ which ssh
# returns:
#   /usr/bin/ssh

tom@hd-single:~$ which sshd
# returns: 
#   /usr/sbin/sshd

# these look ok
{% endhighlight %}

### Create Hadoop User
Create a dedicated Hadoop group **hadoop** and user **hduser**. Hadoop will run under this user and this user will be used by Hadoop to access future nodes with SSH.
{% highlight bash %}
# create group
tom@hd-single:~$ sudo addgroup hadoop

# create user and add user to group
tom@hd-single:~$ sudo adduser --ingroup hadoop hduser

# add the user to the sudoers (administrators) group
tom@hd-single:~$ sudo adduser hduser sudo
{% endhighlight %}

### Create SSH Certificates
Since this is a single node setup, SSH only needs configuration for local host. SSH needs to be up and running and configured to allow SSH public key authentication. In other words, a certificate needs to be created so **hduser** is not prompted for a password.
{% highlight bash %}
# login as hduser
tom@hd-single:~$ su hduser

# back to root
hduser@hd-single:/home/tom$ cd 

# generate public key, leaving all prompts "blank" for this purpose
# this will create public key /home/hduser/.ssh/id_rsa.pub
hduser@hd-single:~$ ssh-keygen -t rsa -P ""

# add the new key to the list of authorized keys
hduser@hd-single:~$ cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys

# check if ssh works, enter "yes" when prompted "Are you sure...?"
hduser@hd-single:~$ ssh localhost
# returns something like:
#   The authenticity of host 'localhost (127.0.0.1)' can't be established.
#   ECDSA key fingerprint is e1:8b:a0:a5:75:ef:f4:b4:5e:a9:ed:be:64:be:5c:2f.
#   Are you sure you want to continue connecting (yes/no)? yes
#   Warning: Permanently added 'localhost' (ECDSA) to the list of known hosts.
#   Welcome to Ubuntu 14.04.1 LTS (GNU/Linux 3.13.0-40-generic x86_64)

{% endhighlight %}

## Install Hadoop
The system is now ready to install Hadoop. I installed Hadoop to **/usr/local/hadoop**.
{% highlight bash %}
# download Hadoop
hduser@hd-single:~$ wget http://mirrors.sonic.net/apache/hadoop/common/hadoop-2.6.0/hadoop-2.6.0.tar.gz

# untar (unzip)
hduser@hd-single:~$ tar xvzf hadoop-2.6.0.tar.gz

# make hadoop directory
hduser@hd-single:~$ sudo mkdir /usr/local/hadoop

# change directory
hduser@hd-single:~$ cd hadoop-2.6.0

# move the contents of Hadoop installation to the /usr/local/hadoop directory:
hduser@hd-single:~/hadoop-2.6.0$ sudo mv * /usr/local/hadoop

# change owner of the hadoop directory
hduser@hd-single:~/hadoop-2.6.0$ sudo chown -R hduser:hadoop /usr/local/hadoop

# back to root
hduser@hd-single:~/hadoop-2.6.0$ cd ..

{% endhighlight %}

##Configure Hadoop
The following files must be modified to complete the Hadoop setup.

#### ~/.bashrc
{% highlight bash %}
# find the path where Java resides to set the JAVA_HOME environment variable
hduser@hd-single:~$ update-alternatives --config java
# returns something like:
#   There is only one alternative in link group java (providing /usr/bin/java): 
#   /usr/lib/jvm/java-7-openjdk-amd64/jre/bin/java
#   Nothing to configure.

# make note path for JAVA_HOME

# edit file
hduser@hd-single:~$ nano ~/.bashrc

# append the following lines to ~/.bashrc
# this will set the environment when the user logs into Ubuntu
#   ensure JAVA_HOME is set correctly with info from alternatives info above
#   ensure HADOOP_HOME is set correctly with path from install
  # HADOOP VARIABLES START
    export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64
    export HADOOP_HOME=/usr/local/hadoop

    export PATH=$PATH:$HADOOP_HOME/bin
    export PATH=$PATH:$HADOOP_HOME/sbin
    export HADOOP_MAPRED_HOME=$HADOOP_HOME
    export HADOOP_COMMON_HOME=$HADOOP_HOME
    export HADOOP_HDFS_HOME=$HADOOP_HOME
    export YARN_HOME=$HADOOP_HOME
    export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
    export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"
  # HADOOP VARIABLES END

# save ~/.bashrc

# if other users will be using hadoop this will need to be repeated for each

# re-set active environment after adding above
hduser@hd-single:~$ source ~/.bashrc

# check environment variables:
hduser@hd-single:~$ printenv
{% endhighlight %}

#### hadoop-env.sh
{% highlight bash %}
# edit file
hduser@hd-single:~$ nano /usr/local/hadoop/etc/hadoop/hadoop-env.sh

# modify following line to reflect correct path for JAVA:
    export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64

# save
# this will ensure JAVA is available to Hadoop at startup
{% endhighlight %}

#### core-site.xml
This file contains configuration properties for Hadoop at startup. This file overrides default settings.
{% highlight bash %}
# create a directory for hadoop to override settings
hduser@hd-single:~$ sudo mkdir -p /app/hadoop/tmp

# change owner
hduser@hd-single:~$ sudo chown hduser:hadoop /app/hadoop/tmp

# edit file
hduser@hd-single:~$ nano /usr/local/hadoop/etc/hadoop/core-site.xml

# set the following property:
<configuration>

  <property>
    <name>fs.default.name</name>
    <value>hdfs://localhost:9000</value>
  </property>

</configuration>

# save
{% endhighlight %}

#### mapred-site.xml
By default, the /usr/local/hadoop/etc/hadoop/ folder contains /usr/local/hadoop/etc/hadoop/mapred-site.xml.template file which must be renamed/copied with the name mapred-site.xml. The mapred-site.xml file is used to specify which framework is being used for MapReduce. We will use the YARN framework.
{% highlight bash %}
# copy
hduser@hd-single:~$ cp /usr/local/hadoop/etc/hadoop/mapred-site.xml.template /usr/local/hadoop/etc/hadoop/mapred-site.xml

# edit file
hduser@hd-single:~$ nano /usr/local/hadoop/etc/hadoop/mapred-site.xml

# set the following property:
<configuration>

  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>

</configuration>

# save
{% endhighlight %}

#### hdfs-site.xml
{% highlight bash %}
# create two directories which will contain
# the namenode and the datanode for this installation
hduser@hd-single:~$ sudo mkdir -p /usr/local/hadoop_store/hdfs/namenode
hduser@hd-single:~$ sudo mkdir -p /usr/local/hadoop_store/hdfs/datanode

# change owner of the store
hduser@hd-single:~$ sudo chown -R hduser:hadoop /usr/local/hadoop_store

# edit file
hduser@hd-single:~$ nano /usr/local/hadoop/etc/hadoop/hdfs-site.xml

# set the following properties:
<configuration>

  <property>
    <name>dfs.replication</name>
    <value>1</value>
  </property>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:/usr/local/hadoop_store/hdfs/namenode</value>
  </property>
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>file:/usr/local/hadoop_store/hdfs/datanode</value>
  </property>
  <property>
    <name>dfs.webhdfs.enabled</name>
    <value>true</value>
  </property>

</configuration>

# save
{% endhighlight %}

#### yarn-site.xml
{% highlight bash %}
# edit file
hduser@hd-single:~$ nano /usr/local/hadoop/etc/hadoop/yarn-site.xml

# set the following property:
<configuration>

  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
  </property>

</configuration>

# save
{% endhighlight %}

## Format Hadoop Filesystem
The format command should be issued with write permission since it creates directory under /usr/local/hadoop_store/hdfs/namenode folder.
{% highlight bash %}
# format HDFS
hduser@hd-single:~$ hadoop namenode -format
{% endhighlight %}

## Start Hadoop and Verify
{% highlight bash %}
# start hdfs:
hduser@hd-single:~$ start-dfs.sh

# may see a warning 
# (WARN util.NativeCodeLoader: Unable....using builtin-java classes where applicable)
# this is ok

# start MapReduce (running on the Yarn)
hduser@hd-single:~$ start-yarn.sh

# verify all daemons
hduser@hd-single:/usr/local/hadoop/sbin$ jps
# should see:
#   Jps
#   NameNode
#   SecondaryNameNode
#   ResourceManager
#   DataNode
#   NodeManager

# check Hadoop version
hduser@hd-single:~$ hadoop version
# returns:
#   Hadoop 2.6.0
#   Subversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r 
#   e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1
#   Compiled by jenkins on 2014-11-13T21:10Z
#   Compiled with protoc 2.5.0
#   From source with checksum 18e43357c8f927c0695f1e9522859d6a
#   This command was run using /usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0.jar
{% endhighlight %}

## Hadoop Browser Interfaces
YARN must be active to access the following default URLs.

|Administration|http://hd-single:8088/|
|NameNode Status|http://hd-single:50070/dfshealth.html#tab-overview|
|DataNode Status|http://hd-single:50070/dfshealth.html#tab-datanode|

Administration
<figure><a href="/images/hd-single-01.png"><img src="/images/hd-single-01.png"></a>
<figcaption></figcaption>
</figure>

NameNode Status
<figure><a href="/images/hd-single-02.png"><img src="/images/hd-single-02.png"></a>
<figcaption></figcaption>
</figure>

DataNode Status
<figure><a href="/images/hd-single-03.png"><img src="/images/hd-single-03.png"></a>
<figcaption></figcaption>
</figure>

This completes the installation.

